{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e1f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Tuple, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f79023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4080e4e0",
   "metadata": {},
   "source": [
    "# RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6725e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angles(theeta, dim, seq_len):\n",
    "    pos = 1/theeta**(torch.arange(0, dim, 2, device=device).float()/dim)\n",
    "    t = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(t, pos)\n",
    "    unit_vecs = torch.polar(torch.ones_like(freqs), freqs).to(device)\n",
    "    return unit_vecs\n",
    "\n",
    "def brodcast(unit_vecs, x):\n",
    "    # print(f\"unit_vecs.shape: {unit_vecs.shape}\\nX.shape: {x.shape[1], x.shape[-1]}\")\n",
    "    assert unit_vecs.shape == (x.shape[1], x.shape[-1])\n",
    "    n_dim = x.ndim\n",
    "    shape = [d if i == 1 or i == n_dim-1 else 1 for i,d in enumerate(x.shape)]\n",
    "    return unit_vecs.view(*shape)\n",
    "\n",
    "def RoPE(W_Q, W_K, unit_vecs):\n",
    "    complex_W_Q = torch.view_as_complex(W_Q.float().reshape(*W_Q.shape[:-1], -1, 2)).to(device)\n",
    "    complex_W_K = torch.view_as_complex(W_K.float().reshape(*W_K.shape[:-1], -1, 2)).to(device)\n",
    "    # print(complex_W_Q.shape)\n",
    "    pos = brodcast(unit_vecs, complex_W_K).to(device)\n",
    "    embedded_W_Q = torch.view_as_real(complex_W_Q * pos).float().flatten(3)\n",
    "    embedded_W_K = torch.view_as_real(complex_W_K * pos).float().flatten(3)\n",
    "    return embedded_W_Q, embedded_W_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b08c6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_Q = torch.rand((1,6,2,4))\n",
    "complex_W_Q = torch.view_as_complex(W_Q.float().reshape(*W_Q.shape[:-1], -1, 2))\n",
    "complex_W_Q.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ccd32f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM = 3072 # Keep as 3072 to match checkpoint\n",
    "FFN_DIM = 8192 # Change from 14336 to 8192 to match checkpoint feed-forward layers\n",
    "N_LAYERS = 28 # This should match your checkpoint's number of layers (0-27 = 28 layers)\n",
    "N_HEADS = 24 # Change from 32 - calculated as DIM/HEAD_DIM where HEAD_DIM=128\n",
    "N_KV_HEADS = 8 # Keep as 8 (matches checkpoint key-value head dimension 1024/128)\n",
    "VOCAB_SIZE = 128256 # Keep same - matches checkpoint\n",
    "NORM_EPS = 1e-5 # Keep same\n",
    "ROPE_THETA = 500000 # Keep same\n",
    "MAX_BATCH_SIZE = 4 # Keep same\n",
    "MAX_SEQ_LEN = 128 # Keep same\n",
    "N_KV_HEAD_REP = N_HEADS // N_KV_HEADS # Now 24 // 8 = 3\n",
    "HEAD_DIM = DIM // N_HEADS # Now 3072 // 24 = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d678e25e",
   "metadata": {},
   "source": [
    "# ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fe84216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = calculate_angles(10000, HEAD_DIM, 2).to(device)\n",
    "freq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64b01afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Linear(DIM, N_HEADS * HEAD_DIM, bias=False).to(device)\n",
    "        self.W_K = nn.Linear(DIM, N_KV_HEADS * HEAD_DIM, bias=False).to(device)\n",
    "        self.W_V = nn.Linear(DIM, N_KV_HEADS * HEAD_DIM, bias=False).to(device)\n",
    "\n",
    "        self.CACHE_K = torch.zeros(\n",
    "            (MAX_BATCH_SIZE, MAX_SEQ_LEN, N_KV_HEADS, HEAD_DIM)\n",
    "        )\n",
    "        self.CACHE_V = torch.zeros(\n",
    "            (MAX_BATCH_SIZE, MAX_SEQ_LEN, N_KV_HEADS, HEAD_DIM)\n",
    "        )\n",
    "\n",
    "        self.wo = nn.Linear(DIM, DIM).to(device)\n",
    "\n",
    "\n",
    "    def forward(self,x, freq=None, start_pos=0, mask=None):\n",
    "        bhz, seq_len, _ = x.shape\n",
    "\n",
    "        query = self.W_Q(x).view(bhz, seq_len, N_HEADS, HEAD_DIM).to(device)\n",
    "        key = self.W_K(x).view(bhz, seq_len, N_KV_HEADS, HEAD_DIM)\n",
    "        value = self.W_V(x).view(bhz, seq_len, N_KV_HEADS, HEAD_DIM)\n",
    "\n",
    "        query, key = RoPE(query, key, freq)\n",
    "\n",
    "        self.CACHE_K.to(device)\n",
    "        self.CACHE_V.to(device)\n",
    "\n",
    "        self.CACHE_K[:bhz, start_pos:start_pos+seq_len] = key\n",
    "        self.CACHE_V[:bhz, start_pos:start_pos+seq_len] = value\n",
    "\n",
    "        keys = self.CACHE_K[:bhz, :start_pos+seq_len]\n",
    "        values = self.CACHE_V[:bhz, :start_pos+seq_len]\n",
    "\n",
    "        keys = torch.repeat_interleave(input=keys, repeats=N_KV_HEAD_REP, dim=-2)\n",
    "        values = torch.repeat_interleave(input=values, repeats=N_KV_HEAD_REP, dim=-2)\n",
    "\n",
    "        queries = query.transpose(1,2).to(device)\n",
    "        keys = keys.transpose(1,2).to(device)\n",
    "        values = values.transpose(1,2).to(device)\n",
    "        \n",
    "        out = F.scaled_dot_product_attention(queries, keys, values, attn_mask=mask)\n",
    "        out = out.transpose(1,2).contiguous().view(bhz, seq_len, -1)\n",
    "\n",
    "        return self.wo(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd9fc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, norm_eps):\n",
    "        super().__init__()\n",
    "        self.norm_eps = norm_eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim)).to(device)\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.norm_eps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self._norm(x.float()).type_as(x).to(device)\n",
    "        return out * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e8997b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w1 = nn.Linear(DIM, FFN_DIM, bias=False).to(device)\n",
    "        self.w3 = nn.Linear(DIM, FFN_DIM, bias=False).to(device)\n",
    "        self.w2 = nn.Linear(FFN_DIM, DIM, bias=False).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fd82bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.Attention_Norm = RMSNorm(dim=DIM, norm_eps=NORM_EPS)\n",
    "        self.FFN_Norm = RMSNorm(dim=DIM, norm_eps=NORM_EPS)\n",
    "        self.Attention = Attention()\n",
    "        self.FeedForward = FeedForward()\n",
    "    def forward(self, x, freq, start_pos, mask):\n",
    "        shortcut = x\n",
    "        x = self.Attention_Norm(x)\n",
    "        x = self.Attention(x, freq, start_pos, mask)\n",
    "        x = x.to(shortcut.device) + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.FFN_Norm(x.to(device))\n",
    "        x = self.FeedForward(x)\n",
    "        x = x.to(shortcut.device) + shortcut\n",
    "\n",
    "        return x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0af5202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLAMA_3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(VOCAB_SIZE, DIM)\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(N_LAYERS):\n",
    "            self.layers.append(Transformer_Block())\n",
    "        self.norm = RMSNorm(DIM, NORM_EPS)\n",
    "        self.output = nn.Linear(DIM, VOCAB_SIZE, bias=False).to(device)\n",
    "\n",
    "        self.freq = calculate_angles(\n",
    "            ROPE_THETA,\n",
    "            HEAD_DIM,\n",
    "            MAX_SEQ_LEN * 2\n",
    "        )\n",
    "    \n",
    "    def forward(self, tokens, start_pos):\n",
    "        bhz, seq_len = tokens.shape\n",
    "        x = self.tok_embedding(tokens)\n",
    "        freq = self.freq[start_pos : start_pos+seq_len]\n",
    "\n",
    "        mask = None\n",
    "        if seq_len > 1:\n",
    "            mask = torch.full((seq_len, seq_len), float(\"-inf\"), device=tokens.device) \n",
    "            mask = torch.triu(mask, diagonal=1).to(device)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, freq, start_pos, mask )\n",
    "        x = self.norm(x)\n",
    "        x = self.output(x).float()\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83e91f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = LLAMA_3().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "658010aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of parameters: 3,606,663,168\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in llama.parameters())\n",
    "print(f\"Total Number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7bae4694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 13.435867309570312 GB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4 #A\n",
    "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
    "print(f\"Total size of the model: {total_size_mb/1024} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dfc77b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_Text(model, idx, max_tokens, context_size, start_pos):\n",
    "    for _ in range(max_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.inference_mode():\n",
    "            logits = model(idx_cond, start_pos)\n",
    "\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_idx = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "            idx = torch.cat((idx, next_idx), dim=-1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8959ff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    0,     0,     0,     0, 41203, 11803, 23389, 12947]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = torch.rand((1,4)).long().to(device)\n",
    "\n",
    "f = Generate_Text(llama, inp, 4, 4, 0)\n",
    "f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

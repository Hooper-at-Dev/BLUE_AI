{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23e1f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "from pathlib import Path\n",
    "from typing import (\n",
    "    AbstractSet,\n",
    "    cast,\n",
    "    Collection,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Literal,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4f79023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4080e4e0",
   "metadata": {},
   "source": [
    "# RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6725e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angles(theeta, dim, seq_len):\n",
    "    pos = 1/theeta**(torch.arange(0, dim, 2, device=device).float()/dim)\n",
    "    t = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(t, pos)\n",
    "    unit_vecs = torch.polar(torch.ones_like(freqs), freqs).to(device)\n",
    "    return unit_vecs\n",
    "\n",
    "def brodcast(unit_vecs, x):\n",
    "    # print(f\"unit_vecs.shape: {unit_vecs.shape}\\nX.shape: {x.shape[1], x.shape[-1]}\")\n",
    "    assert unit_vecs.shape == (x.shape[1], x.shape[-1])\n",
    "    n_dim = x.ndim\n",
    "    shape = [d if i == 1 or i == n_dim-1 else 1 for i,d in enumerate(x.shape)]\n",
    "    return unit_vecs.view(*shape)\n",
    "\n",
    "def RoPE(W_Q, W_K, unit_vecs):\n",
    "    complex_W_Q = torch.view_as_complex(W_Q.float().reshape(*W_Q.shape[:-1], -1, 2)).to(device)\n",
    "    complex_W_K = torch.view_as_complex(W_K.float().reshape(*W_K.shape[:-1], -1, 2)).to(device)\n",
    "    # print(complex_W_Q.shape)\n",
    "    pos = brodcast(unit_vecs, complex_W_K).to(device)\n",
    "    embedded_W_Q = torch.view_as_real(complex_W_Q * pos).float().flatten(3)\n",
    "    embedded_W_K = torch.view_as_real(complex_W_K * pos).float().flatten(3)\n",
    "    return embedded_W_Q, embedded_W_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6b08c6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 2, 2])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_Q = torch.rand((1,6,2,4))\n",
    "complex_W_Q = torch.view_as_complex(W_Q.float().reshape(*W_Q.shape[:-1], -1, 2))\n",
    "complex_W_Q.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9f6c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGURATIONS = {\n",
    "    \"DIM\": 3072,\n",
    "    \"FFN_DIM\": 8192,\n",
    "    \"N_LAYERS\": 28,\n",
    "    \"N_HEADS\": 24,\n",
    "    \"N_KV_HEADS\": 8,\n",
    "    \"VOCAB_SIZE\": 128256,\n",
    "    \"NORM_EPS\": 1e-5,\n",
    "    \"ROPE_THETA\": 500000,\n",
    "    \"MAX_BATCH_SIZE\": 4,\n",
    "    \"MAX_SEQ_LEN\": 128,\n",
    "    \"N_KV_HEAD_REP\": 24 // 8,\n",
    "    \"HEAD_DIM\": 3072 // 24\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d678e25e",
   "metadata": {},
   "source": [
    "# ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0fe84216",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq = calculate_angles(10000, CONFIGURATIONS[\"HEAD_DIM\"], 2).to(device)\n",
    "freq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "64b01afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, head_dim, n_kv_heads, n_kv_heads_reps, max_batch_size, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Linear(dim, n_heads * head_dim, bias=False).to(device)\n",
    "        self.W_K = nn.Linear(dim, n_kv_heads * head_dim, bias=False).to(device)\n",
    "        self.W_V = nn.Linear(dim, n_kv_heads * head_dim, bias=False).to(device)\n",
    "\n",
    "        self.CACHE_K = torch.zeros(\n",
    "            (max_batch_size, max_seq_len, n_kv_heads, head_dim)\n",
    "        )\n",
    "        self.CACHE_V = torch.zeros(\n",
    "            (max_batch_size, max_seq_len, n_kv_heads, head_dim)\n",
    "        )\n",
    "\n",
    "        self.wo = nn.Linear(dim, dim).to(device)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_kv_heads_reps = n_kv_heads_reps\n",
    "\n",
    "\n",
    "    def forward(self,x, freq=None, start_pos=0, mask=None):\n",
    "        bhz, seq_len, _ = x.shape\n",
    "\n",
    "        query = self.W_Q(x).view(bhz, seq_len, self.n_heads, self.head_dim).to(device)\n",
    "        key = self.W_K(x).view(bhz, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        value = self.W_V(x).view(bhz, seq_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        query, key = RoPE(query, key, freq)\n",
    "\n",
    "        self.CACHE_K.to(device)\n",
    "        self.CACHE_V.to(device)\n",
    "\n",
    "        self.CACHE_K[:bhz, start_pos:start_pos+seq_len] = key\n",
    "        self.CACHE_V[:bhz, start_pos:start_pos+seq_len] = value\n",
    "\n",
    "        keys = self.CACHE_K[:bhz, :start_pos+seq_len]\n",
    "        values = self.CACHE_V[:bhz, :start_pos+seq_len]\n",
    "\n",
    "        keys = torch.repeat_interleave(input=keys, repeats=self.n_kv_heads_reps, dim=-2)\n",
    "        values = torch.repeat_interleave(input=values, repeats=self.n_kv_heads_reps, dim=-2)\n",
    "\n",
    "        queries = query.transpose(1,2).to(device)\n",
    "        keys = keys.transpose(1,2).to(device)\n",
    "        values = values.transpose(1,2).to(device)\n",
    "        \n",
    "        out = F.scaled_dot_product_attention(queries, keys, values, attn_mask=mask)\n",
    "        out = out.transpose(1,2).contiguous().view(bhz, seq_len, -1)\n",
    "\n",
    "        return self.wo(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd9fc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, norm_eps):\n",
    "        super().__init__()\n",
    "        self.norm_eps = norm_eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim)).to(device)\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.norm_eps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self._norm(x.float()).type_as(x).to(device)\n",
    "        return out * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e8997b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w1 = nn.Linear(dim, ffn_dim, bias=False).to(device)\n",
    "        self.w3 = nn.Linear(dim, ffn_dim, bias=False).to(device)\n",
    "        self.w2 = nn.Linear(ffn_dim, dim, bias=False).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4fd82bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Attention_Norm = RMSNorm(dim=config[\"DIM\"], norm_eps=config[\"NORM_EPS\"])\n",
    "        self.FFN_Norm = RMSNorm(dim=config[\"DIM\"], norm_eps=config[\"NORM_EPS\"])\n",
    "        self.Attention = Attention(dim=config[\"DIM\"],\n",
    "                                   n_heads=config[\"N_HEADS\"],\n",
    "                                   head_dim=config[\"HEAD_DIM\"],\n",
    "                                   n_kv_heads=config[\"N_KV_HEADS\"],\n",
    "                                   n_kv_heads_reps=config[\"N_KV_HEAD_REP\"],\n",
    "                                   max_batch_size=config[\"MAX_BATCH_SIZE\"],\n",
    "                                   max_seq_len=config[\"MAX_SEQ_LEN\"])\n",
    "        self.FeedForward = FeedForward(dim=config[\"DIM\"],\n",
    "                                       ffn_dim=config[\"FFN_DIM\"])\n",
    "    def forward(self, x, freq, start_pos, mask):\n",
    "        shortcut = x\n",
    "        x = self.Attention_Norm(x)\n",
    "        x = self.Attention(x, freq, start_pos, mask)\n",
    "        x = x.to(shortcut.device) + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.FFN_Norm(x.to(device))\n",
    "        x = self.FeedForward(x)\n",
    "        x = x.to(shortcut.device) + shortcut\n",
    "\n",
    "        return x.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0af5202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLAMA_3(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(config[\"VOCAB_SIZE\"], config[\"DIM\"])\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(config[\"N_LAYERS\"]):\n",
    "            self.layers.append(Transformer_Block(config))\n",
    "        self.norm = RMSNorm(config[\"DIM\"], config[\"NORM_EPS\"])\n",
    "        self.output = nn.Linear(config[\"DIM\"], config[\"VOCAB_SIZE\"], bias=False).to(device)\n",
    "\n",
    "        self.freq = calculate_angles(\n",
    "            config[\"ROPE_THETA\"],\n",
    "            config[\"HEAD_DIM\"],\n",
    "            config[\"MAX_SEQ_LEN\"] * 2\n",
    "        )\n",
    "    \n",
    "    def forward(self, tokens, start_pos):\n",
    "        bhz, seq_len = tokens.shape\n",
    "        x = self.tok_embedding(tokens)\n",
    "        freq = self.freq[start_pos : start_pos+seq_len]\n",
    "\n",
    "        mask = None\n",
    "        if seq_len > 1:\n",
    "            mask = torch.full((seq_len, seq_len), float(\"-inf\"), device=tokens.device) \n",
    "            mask = torch.triu(mask, diagonal=1).to(device)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, freq, start_pos, mask )\n",
    "        x = self.norm(x)\n",
    "        x = self.output(x).float()\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "83e91f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = LLAMA_3(config=CONFIGURATIONS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "658010aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of parameters: 3,606,663,168\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in llama.parameters())\n",
    "print(f\"Total Number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7bae4694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 13.435867309570312 GB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4 #A\n",
    "total_size_mb = total_size_bytes / (1024 * 1024) #B\n",
    "print(f\"Total size of the model: {total_size_mb/1024} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dfc77b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_Text(model, idx, max_tokens, context_size, start_pos):\n",
    "    for _ in range(max_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.inference_mode():\n",
    "            logits = model(idx_cond, start_pos)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_idx = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "            idx = torch.cat((idx, next_idx), dim=-1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c222ee",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    no_reserved_special_tokens = 256\n",
    "    pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E501\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        mergeable_ranks = load_tiktoken_bpe(\"./Weights/Llama-3.2-3B/original/tokenizer.model\")\n",
    "        num_base_tokens = len(mergeable_ranks)\n",
    "        special_tokens = [\n",
    "            \"<|begin_of_text|>\",\n",
    "            \"<|end_of_text|>\",\n",
    "            \"<|reserved_special_token_0|>\",\n",
    "            \"<|reserved_special_token_1|>\",\n",
    "            \"<|reserved_special_token_2|>\",\n",
    "            \"<|reserved_special_token_3|>\",\n",
    "            \"<|start_header_id|>\",\n",
    "            \"<|end_header_id|>\",\n",
    "            \"<|reserved_special_token_4|>\",\n",
    "            \"<|eot_id|>\",  # end of turn\n",
    "        ] + [f\"<|reserved_special_token_{i}|>\" for i in range(5, self.no_reserved_special_tokens - 5)]\n",
    "        \n",
    "        self.special_tokens = {\n",
    "            token: num_base_tokens + i for i, token in enumerate(special_tokens)\n",
    "        }\n",
    "\n",
    "        self.model = tiktoken.Encoding(\n",
    "            name=Path(model_path).name,\n",
    "            pat_str=self.pat_str,\n",
    "            mergeable_ranks=mergeable_ranks,\n",
    "            special_tokens=self.special_tokens\n",
    "        )\n",
    "        self.n_words = self.model.n_vocab\n",
    "        self.bos_id: int = self.special_tokens[\"<|begin_of_text|>\"]\n",
    "        self.eos_id: int = self.special_tokens[\"<|end_of_text|>\"]\n",
    "        self.pad_id: int = -1\n",
    "        self.stop_tokens = {\n",
    "            self.special_tokens[\"<|end_of_text|>\"],\n",
    "            self.special_tokens[\"<|eot_id|>\"],\n",
    "        }\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        s: str,\n",
    "        *,\n",
    "        bos: bool,\n",
    "        eos: bool,\n",
    "        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\n",
    "        disallowed_special: Union[Literal[\"all\"], Collection[str]] = (),\n",
    "        ):\n",
    "        TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n",
    "        MAX_NO_WHITESPACES_CHARS = 25_000\n",
    "        substrs = (\n",
    "            substr\n",
    "            for i in range(0, len(s), TIKTOKEN_MAX_ENCODE_CHARS)\n",
    "            for substr in self._split_whitespaces_or_nonwhitespaces(\n",
    "                s[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS\n",
    "            )\n",
    "        )\n",
    "        t: List[int] = []\n",
    "        for substr in substrs:\n",
    "            t.extend(\n",
    "                self.model.encode(\n",
    "                    substr,\n",
    "                    allowed_special=allowed_special,\n",
    "                    disallowed_special=disallowed_special,\n",
    "                )\n",
    "            )\n",
    "        if bos:\n",
    "            t.insert(0, self.bos_id)\n",
    "        if eos:\n",
    "            t.append(self.eos_id)\n",
    "        return t\n",
    "    def decode(self, t: Sequence[int]) -> str:\n",
    "        return self.model.decode(cast(List[int], t))\n",
    "\n",
    "    def _split_whitespaces_or_nonwhitespaces(self,\n",
    "        s: str, max_consecutive_slice_len: int\n",
    "    ):\n",
    "        current_slice_len = 0\n",
    "        current_slice_is_space = s[0].isspace() if len(s) > 0 else False\n",
    "        slice_start = 0\n",
    "\n",
    "        for i in range(len(s)):\n",
    "            is_now_space = s[i].isspace()\n",
    "\n",
    "            if current_slice_is_space ^ is_now_space:\n",
    "                current_slice_len = 1\n",
    "                current_slice_is_space = is_now_space\n",
    "            else:\n",
    "                current_slice_len += 1\n",
    "                if current_slice_len > max_consecutive_slice_len:\n",
    "                    yield s[slice_start:i]\n",
    "                    slice_start = i\n",
    "                    current_slice_len = 1\n",
    "        yield s[slice_start:]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3ea995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(\"Weights/Llama-3.2-3B/original/tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a195f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tok.encode(s=\"my name is sarthak\", bos=False, eos=False)\n",
    "z = torch.tensor([z], dtype=torch.long, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4cf5040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = Generate_Text(llama, z, 5, 10, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f37b15ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = d.squeeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "4c6cb307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is sarthak_EXTENSIONSlerineВін المفatoon'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.decode(d.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b760b",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a59f772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20480"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIR = \"./DATA/the-verdict.txt\"\n",
    "\n",
    "with open(DIR, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cfa930",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aef30ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_sample(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_token_seen = [], [], []\n",
    "\n",
    "    token_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        batch_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        for input_batch, target_batch in batch_iter:\n",
    "            optimizer.zero_grad()\n",
    "            loss = corss_entropy_loss(input_batch, target_batch, device, model)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            token_seen += input_batch.numel()\n",
    "            global_step +=1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_token_seen.append(token_seen)\n",
    "                print(f\"Ep {epoch+1}(Step {global_step:06d}): \"\n",
    "                      f\"Train Loss: {train_loss:.3f}, val Loss: {val_loss:.3f}\")\n",
    "                \n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "\n",
    "    return train_losses, val_losses, track_token_seen\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

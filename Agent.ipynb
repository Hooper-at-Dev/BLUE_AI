{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "23e1f3bf",
      "metadata": {
        "id": "23e1f3bf"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple, Dict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import tiktoken\n",
        "from tiktoken.load import load_tiktoken_bpe\n",
        "from pathlib import Path\n",
        "from typing import (\n",
        "    AbstractSet,\n",
        "    cast,\n",
        "    Collection,\n",
        "    Dict,\n",
        "    Iterator,\n",
        "    List,\n",
        "    Literal,\n",
        "    Sequence,\n",
        "    TypedDict,\n",
        "    Union,\n",
        ")\n",
        "import json\n",
        "import numpy as np\n",
        "from torch.nn.attention import sdpa_kernel, SDPBackend\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Projects/llama3_2_8B_instruct_weights')"
      ],
      "metadata": {
        "id": "9tVEovelmrlZ"
      },
      "id": "9tVEovelmrlZ",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "4f79023f",
      "metadata": {
        "id": "4f79023f"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsCsl07fcVKy",
        "outputId": "6f2f4c05-00ab-45f4-d9bf-afdd91776a9e"
      },
      "id": "fsCsl07fcVKy",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4080e4e0",
      "metadata": {
        "id": "4080e4e0"
      },
      "source": [
        "# RoPE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "f6725e9a",
      "metadata": {
        "id": "f6725e9a"
      },
      "outputs": [],
      "source": [
        "def calculate_angles(theeta, dim, seq_len):\n",
        "    pos = 1/theeta**(torch.arange(0, dim, 2, device=device).float()/dim)\n",
        "    t = torch.arange(seq_len, device=device)\n",
        "    freqs = torch.outer(t, pos)\n",
        "    unit_vecs = torch.polar(torch.ones_like(freqs), freqs).to(device)\n",
        "    return unit_vecs\n",
        "\n",
        "def brodcast(unit_vecs, x):\n",
        "    # print(f\"unit_vecs.shape: {unit_vecs.shape}\\nX.shape: {x.shape[1], x.shape[-1]}\")\n",
        "    assert unit_vecs.shape == (x.shape[1], x.shape[-1])\n",
        "    n_dim = x.ndim\n",
        "    shape = [d if i == 1 or i == n_dim-1 else 1 for i,d in enumerate(x.shape)]\n",
        "    return unit_vecs.view(*shape)\n",
        "\n",
        "def RoPE(W_Q, W_K, unit_vecs):\n",
        "    complex_W_Q = torch.view_as_complex(W_Q.float().reshape(*W_Q.shape[:-1], -1, 2)).to(device)\n",
        "    complex_W_K = torch.view_as_complex(W_K.float().reshape(*W_K.shape[:-1], -1, 2)).to(device)\n",
        "    # print(complex_W_Q.shape)\n",
        "    pos = brodcast(unit_vecs, complex_W_K).to(device)\n",
        "    embedded_W_Q = torch.view_as_real(complex_W_Q * pos).float().flatten(3)\n",
        "    embedded_W_K = torch.view_as_real(complex_W_K * pos).float().flatten(3)\n",
        "    return embedded_W_Q, embedded_W_K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d9f6c1c8",
      "metadata": {
        "id": "d9f6c1c8"
      },
      "outputs": [],
      "source": [
        "CONFIGURATIONS = {\n",
        "    \"DIM\": 3072,\n",
        "    \"FFN_DIM\": 8192,\n",
        "    \"N_LAYERS\": 28,\n",
        "    \"N_HEADS\": 24,\n",
        "    \"N_KV_HEADS\": 8,\n",
        "    \"VOCAB_SIZE\": 128256,\n",
        "    \"NORM_EPS\": 1e-5,\n",
        "    \"ROPE_THETA\": 500000,\n",
        "    \"MAX_BATCH_SIZE\": 4,\n",
        "    \"MAX_SEQ_LEN\": 4096,\n",
        "    \"N_KV_HEAD_REP\": 24 // 8,\n",
        "    \"HEAD_DIM\": 3072 // 24\n",
        "    }\n",
        "\n",
        "\n",
        "# CONFIGURATIONS = {\n",
        "#     'DIM': 4096,\n",
        "#     'FFN_DIM': 14336,\n",
        "#     'N_LAYERS': 32,\n",
        "#     'N_HEADS': 32,\n",
        "#     'N_KV_HEADS': 8,\n",
        "#     'VOCAB_SIZE': 128256,\n",
        "#     'NORM_EPS': 1e-05,\n",
        "#     'ROPE_THETA': 500000.0,\n",
        "#     'MAX_BATCH_SIZE': 4,\n",
        "#     'MAX_SEQ_LEN': 2048,\n",
        "#     'N_KV_HEAD_REP': 4,\n",
        "#     'HEAD_DIM': 128\n",
        "# }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d678e25e",
      "metadata": {
        "id": "d678e25e"
      },
      "source": [
        "# LAYERS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "64b01afc",
      "metadata": {
        "id": "64b01afc"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, n_heads, head_dim, n_kv_heads, n_kv_heads_reps, max_batch_size, max_seq_len):\n",
        "        super().__init__()\n",
        "        self.W_Q = nn.Linear(dim, n_heads * head_dim, bias=False)\n",
        "        self.W_K = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n",
        "        self.W_V = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n",
        "\n",
        "        self.CACHE_K = torch.zeros(\n",
        "            (max_batch_size, max_seq_len, n_kv_heads, head_dim),\n",
        "            device=device\n",
        "        )\n",
        "        self.CACHE_V = torch.zeros(\n",
        "            (max_batch_size, max_seq_len, n_kv_heads, head_dim),\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        self.wo = nn.Linear(dim, dim)\n",
        "\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.n_kv_heads = n_kv_heads\n",
        "        self.n_kv_heads_reps = n_kv_heads_reps\n",
        "\n",
        "\n",
        "    def forward(self,x, freq=None, start_pos=0, mask=None):\n",
        "        bhz, seq_len, _ = x.shape\n",
        "\n",
        "        query = self.W_Q(x).view(bhz, seq_len, self.n_heads, self.head_dim)\n",
        "        key = self.W_K(x).view(bhz, seq_len, self.n_kv_heads, self.head_dim)\n",
        "        value = self.W_V(x).view(bhz, seq_len, self.n_kv_heads, self.head_dim)\n",
        "\n",
        "        query, key = RoPE(query, key, freq)\n",
        "\n",
        "        # self.CACHE_K.to(device)\n",
        "        # self.CACHE_V.to(device)\n",
        "\n",
        "        self.CACHE_K[:bhz, start_pos:start_pos+seq_len] = key\n",
        "        self.CACHE_V[:bhz, start_pos:start_pos+seq_len] = value\n",
        "\n",
        "        keys = self.CACHE_K[:bhz, :start_pos+seq_len]\n",
        "        values = self.CACHE_V[:bhz, :start_pos+seq_len]\n",
        "\n",
        "        keys = torch.repeat_interleave(input=keys, repeats=self.n_kv_heads_reps, dim=-2)\n",
        "        values = torch.repeat_interleave(input=values, repeats=self.n_kv_heads_reps, dim=-2)\n",
        "\n",
        "        queries = query.transpose(1,2)\n",
        "        keys = keys.transpose(1,2)\n",
        "        values = values.transpose(1,2)\n",
        "\n",
        "        out = F.scaled_dot_product_attention(queries, keys, values, attn_mask=mask)\n",
        "        out = out.transpose(1,2).contiguous().view(bhz, seq_len, -1)\n",
        "\n",
        "        return self.wo(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "bd9fc577",
      "metadata": {
        "id": "bd9fc577"
      },
      "outputs": [],
      "source": [
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim, norm_eps):\n",
        "        super().__init__()\n",
        "        self.norm_eps = norm_eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.norm_eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self._norm(x.float()).type_as(x)\n",
        "        return out * self.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8e8997b3",
      "metadata": {
        "id": "8e8997b3"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, ffn_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.w1 = nn.Linear(dim, ffn_dim, bias=False)\n",
        "        self.w3 = nn.Linear(dim, ffn_dim, bias=False)\n",
        "        self.w2 = nn.Linear(ffn_dim, dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4fd82bdc",
      "metadata": {
        "id": "4fd82bdc"
      },
      "outputs": [],
      "source": [
        "class Transformer_Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.Attention_Norm = RMSNorm(dim=config[\"DIM\"], norm_eps=config[\"NORM_EPS\"])\n",
        "        self.FFN_Norm = RMSNorm(dim=config[\"DIM\"], norm_eps=config[\"NORM_EPS\"])\n",
        "        self.Attention = Attention(dim=config[\"DIM\"],\n",
        "                                   n_heads=config[\"N_HEADS\"],\n",
        "                                   head_dim=config[\"HEAD_DIM\"],\n",
        "                                   n_kv_heads=config[\"N_KV_HEADS\"],\n",
        "                                   n_kv_heads_reps=config[\"N_KV_HEAD_REP\"],\n",
        "                                   max_batch_size=config[\"MAX_BATCH_SIZE\"],\n",
        "                                   max_seq_len=config[\"MAX_SEQ_LEN\"])\n",
        "        self.FeedForward = FeedForward(dim=config[\"DIM\"],\n",
        "                                       ffn_dim=config[\"FFN_DIM\"])\n",
        "    def forward(self, x, freq, start_pos, mask):\n",
        "        shortcut = x\n",
        "        x = self.Attention_Norm(x)\n",
        "        x = self.Attention(x, freq, start_pos, mask)\n",
        "        x = x + shortcut\n",
        "\n",
        "        shortcut = x\n",
        "        x = self.FFN_Norm(x)\n",
        "        x = self.FeedForward(x)\n",
        "        x = x + shortcut\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "0af5202f",
      "metadata": {
        "id": "0af5202f"
      },
      "outputs": [],
      "source": [
        "class LLAMA_3(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.tok_embedding = nn.Embedding(config[\"VOCAB_SIZE\"], config[\"DIM\"])\n",
        "        self.layers = nn.ModuleList()\n",
        "        for _ in range(config[\"N_LAYERS\"]):\n",
        "            self.layers.append(Transformer_Block(config))\n",
        "        self.norm = RMSNorm(config[\"DIM\"], config[\"NORM_EPS\"])\n",
        "        self.output = nn.Linear(config[\"DIM\"], config[\"VOCAB_SIZE\"], bias=False)\n",
        "\n",
        "        self.freq = calculate_angles(\n",
        "            config[\"ROPE_THETA\"],\n",
        "            config[\"HEAD_DIM\"],\n",
        "            config[\"MAX_SEQ_LEN\"] * 2\n",
        "        )\n",
        "\n",
        "    def forward(self, tokens, start_pos):\n",
        "        bhz, seq_len = tokens.shape\n",
        "        x = self.tok_embedding(tokens)\n",
        "        freq = self.freq[start_pos : start_pos+seq_len]\n",
        "\n",
        "        mask = None\n",
        "        if seq_len > 1:\n",
        "            mask = torch.full((seq_len, seq_len), float(\"-inf\"), device=tokens.device)\n",
        "            mask = torch.triu(mask, diagonal=1)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, freq, start_pos, mask )\n",
        "        x = self.norm(x)\n",
        "        x = self.output(x).float()\n",
        "\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "83e91f71",
      "metadata": {
        "id": "83e91f71"
      },
      "outputs": [],
      "source": [
        "llama = LLAMA_3(config=CONFIGURATIONS).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "658010aa",
      "metadata": {
        "id": "658010aa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15dfbab6-7d4d-4ad3-b47f-3a5aabaf9f96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of parameters: 3,606,838,272\n"
          ]
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in llama.parameters())\n",
        "print(f\"Total Number of parameters: {total_params:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7bae4694",
      "metadata": {
        "id": "7bae4694",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2d307e6-aaf5-46e3-9ba8-b096b78e3b04"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total size of the model: 13.436519622802734 GB\n"
          ]
        }
      ],
      "source": [
        "total_size_bytes = total_params * 4\n",
        "total_size_mb = total_size_bytes / (1024 * 1024)\n",
        "print(f\"Total size of the model: {total_size_mb/1024} GB\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "dfc77b1c",
      "metadata": {
        "id": "dfc77b1c"
      },
      "outputs": [],
      "source": [
        "def Generate_Text(model, idx, max_tokens, context_size, start_pos):\n",
        "    for _ in range(max_tokens):\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "        with torch.inference_mode():\n",
        "            logits = model(idx_cond, start_pos)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "            next_idx = torch.argmax(probs, dim=-1, keepdim=True)\n",
        "            idx = torch.cat((idx, next_idx), dim=-1)\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41c222ee",
      "metadata": {
        "id": "41c222ee"
      },
      "source": [
        "# Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2d47063c",
      "metadata": {
        "id": "2d47063c"
      },
      "outputs": [],
      "source": [
        "class Tokenizer:\n",
        "    no_reserved_special_tokens = 256\n",
        "    pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E501\n",
        "\n",
        "    def __init__(self, model_path):\n",
        "        mergeable_ranks = load_tiktoken_bpe(model_path)\n",
        "        num_base_tokens = len(mergeable_ranks)\n",
        "        special_tokens = [\n",
        "            \"<|begin_of_text|>\",\n",
        "            \"<|end_of_text|>\",\n",
        "            \"<|reserved_special_token_0|>\",\n",
        "            \"<|reserved_special_token_1|>\",\n",
        "            \"<|reserved_special_token_2|>\",\n",
        "            \"<|reserved_special_token_3|>\",\n",
        "            \"<|start_header_id|>\",\n",
        "            \"<|end_header_id|>\",\n",
        "            \"<|reserved_special_token_4|>\",\n",
        "            \"<|eot_id|>\",  # end of turn\n",
        "        ] + [f\"<|reserved_special_token_{i}|>\" for i in range(5, self.no_reserved_special_tokens - 5)]\n",
        "\n",
        "        self.special_tokens = {\n",
        "            token: num_base_tokens + i for i, token in enumerate(special_tokens)\n",
        "        }\n",
        "\n",
        "        self.model = tiktoken.Encoding(\n",
        "            name=Path(model_path).name,\n",
        "            pat_str=self.pat_str,\n",
        "            mergeable_ranks=mergeable_ranks,\n",
        "            special_tokens=self.special_tokens\n",
        "        )\n",
        "        self.n_words = self.model.n_vocab\n",
        "        self.bos_id: int = self.special_tokens[\"<|begin_of_text|>\"]\n",
        "        self.eos_id: int = self.special_tokens[\"<|end_of_text|>\"]\n",
        "        self.pad_id: int = -1\n",
        "        self.stop_tokens = {\n",
        "            self.special_tokens[\"<|end_of_text|>\"],\n",
        "            self.special_tokens[\"<|eot_id|>\"],\n",
        "        }\n",
        "\n",
        "    def encode(\n",
        "        self,\n",
        "        s: str,\n",
        "        *,\n",
        "        bos: bool,\n",
        "        eos: bool,\n",
        "        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\n",
        "        disallowed_special: Union[Literal[\"all\"], Collection[str]] = (),\n",
        "        ):\n",
        "        TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n",
        "        MAX_NO_WHITESPACES_CHARS = 25_000\n",
        "        substrs = (\n",
        "            substr\n",
        "            for i in range(0, len(s), TIKTOKEN_MAX_ENCODE_CHARS)\n",
        "            for substr in self._split_whitespaces_or_nonwhitespaces(\n",
        "                s[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS\n",
        "            )\n",
        "        )\n",
        "        t: List[int] = []\n",
        "        for substr in substrs:\n",
        "            t.extend(\n",
        "                self.model.encode(\n",
        "                    substr,\n",
        "                    allowed_special=allowed_special,\n",
        "                    disallowed_special=disallowed_special,\n",
        "                )\n",
        "            )\n",
        "        if bos:\n",
        "            t.insert(0, self.bos_id)\n",
        "        if eos:\n",
        "            t.append(self.eos_id)\n",
        "        return t\n",
        "    def decode(self, t: Sequence[int]) -> str:\n",
        "        return self.model.decode(cast(List[int], t))\n",
        "\n",
        "    def _split_whitespaces_or_nonwhitespaces(self,\n",
        "        s: str, max_consecutive_slice_len: int\n",
        "    ):\n",
        "        current_slice_len = 0\n",
        "        current_slice_is_space = s[0].isspace() if len(s) > 0 else False\n",
        "        slice_start = 0\n",
        "\n",
        "        for i in range(len(s)):\n",
        "            is_now_space = s[i].isspace()\n",
        "\n",
        "            if current_slice_is_space ^ is_now_space:\n",
        "                current_slice_len = 1\n",
        "                current_slice_is_space = is_now_space\n",
        "            else:\n",
        "                current_slice_len += 1\n",
        "                if current_slice_len > max_consecutive_slice_len:\n",
        "                    yield s[slice_start:i]\n",
        "                    slice_start = i\n",
        "                    current_slice_len = 1\n",
        "        yield s[slice_start:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c3ea995f",
      "metadata": {
        "id": "c3ea995f"
      },
      "outputs": [],
      "source": [
        "tok = Tokenizer('/content/drive/MyDrive/Projects/3B-instruct/tokenizer.model')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a195f25a",
      "metadata": {
        "id": "a195f25a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8d4c5a84-8d1f-46d3-804d-834a66faad96"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'my name is sarthak devsrodu charger is&#'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "z = tok.encode(s=\"my name is sarthak\", bos=False, eos=False)\n",
        "z = torch.tensor([z], dtype=torch.long, device=\"cuda\")\n",
        "d = Generate_Text(llama, z, 5, 10, 0)\n",
        "d = d.squeeze(dim=0)\n",
        "tok.decode(d.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "c12110fd",
      "metadata": {
        "id": "c12110fd"
      },
      "outputs": [],
      "source": [
        "from load_gpt_2_355M import load_consolidated_pth_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "388dd73d",
      "metadata": {
        "id": "388dd73d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e69ae991-a6c8-4bc0-f8bf-fa780bb6fe17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded 255 tensors from /content/drive/MyDrive/Projects/3B-instruct/consolidated.00.pth\n"
          ]
        }
      ],
      "source": [
        "dir = \"/content/drive/MyDrive/Projects/3B-instruct/consolidated.00.pth\"\n",
        "params = load_consolidated_pth_weights(dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "1750c39a",
      "metadata": {
        "id": "1750c39a"
      },
      "outputs": [],
      "source": [
        "def weight_injector(my_model, params):\n",
        "    device = next(my_model.parameters()).device\n",
        "    dtype = next(my_model.parameters()).dtype\n",
        "\n",
        "    my_model.tok_embedding.weight.data.copy_(\n",
        "        params[\"tok_embeddings.weight\"].to(device=device, dtype=dtype)\n",
        "    )\n",
        "\n",
        "    num_layers = len(my_model.layers)\n",
        "    for i in range(num_layers):\n",
        "        layer = my_model.layers[i]\n",
        "\n",
        "        layer.Attention.W_Q.weight.data.copy_(\n",
        "            params[f\"layers.{i}.attention.wq.weight\"].to(device=device, dtype=dtype)\n",
        "        )\n",
        "        layer.Attention.W_K.weight.data.copy_(\n",
        "            params[f\"layers.{i}.attention.wk.weight\"].to(device=device, dtype=dtype)\n",
        "        )\n",
        "        layer.Attention.W_V.weight.data.copy_(\n",
        "            params[f\"layers.{i}.attention.wv.weight\"].to(device=device, dtype=dtype)\n",
        "        )\n",
        "        layer.Attention.wo.weight.data.copy_(\n",
        "            params[f\"layers.{i}.attention.wo.weight\"].to(device=device, dtype=dtype)\n",
        "        )\n",
        "\n",
        "        if hasattr(layer.Attention.wo, \"bias\") and f\"layers.{i}.attention.wo.bias\" in params:\n",
        "            layer.Attention.wo.bias.data.copy_(\n",
        "                params[f\"layers.{i}.attention.wo.bias\"].to(device=device, dtype=dtype)\n",
        "            )\n",
        "\n",
        "        layer.FeedForward.w1.weight.data.copy_(\n",
        "            params[f\"layers.{i}.feed_forward.w1.weight\"].to(device=device, dtype=dtype)\n",
        "        )\n",
        "        layer.FeedForward.w3.weight.data.copy_(\n",
        "            params[f\"layers.{i}.feed_forward.w3.weight\"].to(device=device, dtype=dtype)\n",
        "        )\n",
        "        layer.FeedForward.w2.weight.data.copy_(\n",
        "            params[f\"layers.{i}.feed_forward.w2.weight\"].to(device=device, dtype=dtype)\n",
        "        )\n",
        "\n",
        "        layer.Attention_Norm.weight.data.copy_(\n",
        "            params[f\"layers.{i}.attention_norm.weight\"].to(device=device, dtype=dtype)\n",
        "        )\n",
        "        layer.FFN_Norm.weight.data.copy_(\n",
        "            params[f\"layers.{i}.ffn_norm.weight\"].to(device=device, dtype=dtype)\n",
        "        )\n",
        "\n",
        "    my_model.norm.weight.data.copy_(\n",
        "        params[\"norm.weight\"].to(device=device, dtype=dtype)\n",
        "    )\n",
        "\n",
        "    out_w = params[\"output.weight\"]\n",
        "    if my_model.output.weight.shape == out_w.shape:\n",
        "        my_model.output.weight.data.copy_(out_w.to(device=device, dtype=dtype))\n",
        "    elif my_model.output.weight.shape[::-1] == out_w.shape:\n",
        "        my_model.output.weight.data.copy_(out_w.T.to(device=device, dtype=dtype))\n",
        "    else:\n",
        "        raise ValueError(f\"Output weight shape mismatch: model={my_model.output.weight.shape}, ckpt={out_w.shape}\")\n",
        "\n",
        "    print(\"✅ All weights successfully injected into your model!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "f327b0d6",
      "metadata": {
        "id": "f327b0d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e699e4e-4521-4e3f-d349-4a1b60610c80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ All weights successfully injected into your model!\n"
          ]
        }
      ],
      "source": [
        "weight_injector(llama, params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "619e5790",
      "metadata": {
        "id": "619e5790"
      },
      "outputs": [],
      "source": [
        "def generate(\n",
        "    model, idx, context_len=2048, max_new_tok=512,\n",
        "    top_k=10, temp=0.0, eos_id=128009, start_pos=0\n",
        "):\n",
        "    device = idx.device\n",
        "    tok_no = 0\n",
        "\n",
        "    for _ in range(max_new_tok):\n",
        "        print(f\"Generating token {tok_no}...\")\n",
        "        idx_cond = idx[:, -context_len:]\n",
        "\n",
        "        with torch.inference_mode():\n",
        "            logits = model(idx_cond, start_pos)  # [B, T, V]\n",
        "            logits = logits[:, -1, :]  # last token\n",
        "\n",
        "        # top-k filtering\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, top_k)\n",
        "            min_vals = v[:, -1].unsqueeze(-1)\n",
        "            logits = torch.where(logits < min_vals, torch.full_like(logits, -float(\"inf\")), logits)\n",
        "\n",
        "        # sampling\n",
        "        if temp > 0.0:\n",
        "            probs = torch.softmax(logits / temp, dim=-1)\n",
        "            preds = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            preds = torch.argmax(logits, dim=-1, keepdim=True)\n",
        "\n",
        "        # ✅ Proper EOS check\n",
        "        if eos_id is not None and (preds == eos_id).any():\n",
        "            print(\"EOS token encountered — stopping generation.\")\n",
        "            idx = torch.cat((idx, preds), dim=1)\n",
        "            break\n",
        "\n",
        "        # append new token\n",
        "        idx = torch.cat((idx, preds), dim=1)\n",
        "        tok_no += 1\n",
        "\n",
        "    return idx\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "id": "7910ccd5",
      "metadata": {
        "id": "7910ccd5"
      },
      "outputs": [],
      "source": [
        "def format_and_tokenize(system_prompt, user_prompt, context=None, device=\"cuda\"):\n",
        "    if context:\n",
        "        user_prompt = f\"Use the following context to answer:\\n{context.strip()}\\n\\nQuestion: {user_prompt.strip()}\"\n",
        "\n",
        "    prompt = (\n",
        "        \"<|start_header_id|>system<|end_header_id|>\\n\"\n",
        "        f\"{system_prompt.strip()}\\n\"\n",
        "        \"<|eot_id|>\\n\"\n",
        "        \"<|start_header_id|>user<|end_header_id|>\\n\"\n",
        "        f\"{user_prompt.strip()}\\n\"\n",
        "        \"<|eot_id|>\\n\"\n",
        "        \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
        "    )\n",
        "\n",
        "    input_ids = tok.encode(prompt, bos=True, eos=False)\n",
        "    return torch.tensor([input_ids], device=device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "id": "929da2ab",
      "metadata": {
        "id": "929da2ab"
      },
      "outputs": [],
      "source": [
        "system_prompt = \"You are a concise and factual AI assistant. When context is provided, summarize it in your own words rather than copying.\"\n",
        "user_prompt = \"What is sarthaks eConnect?\"\n",
        "context = \"Sarthak’s eConnect is an educational platform designed to support students in preparing for competitive and academic exams in India. It provides a wide range of study resources, including notes, sample papers, quizzes, mock tests, and previous year question papers for exams like JEE, NEET, UPSC, SSC, and state boards.The platform also features an active Q&A community, where students, teachers, and experts can interact—asking and answering academic or exam-related questions. It encourages collaborative learning, doubt-solving, and knowledge sharing. Additionally, users can access daily practice questions, current affairs updates, and topic-wise discussions to enhance their preparation and conceptual understanding.\"\n",
        "idx = format_and_tokenize(system_prompt, user_prompt, context)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "id": "4dda9616",
      "metadata": {
        "id": "4dda9616",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5ef4cd18-e5b7-49e2-b197-274fbcfc415c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating token 0...\n",
            "Generating token 1...\n",
            "Generating token 2...\n",
            "Generating token 3...\n",
            "Generating token 4...\n",
            "Generating token 5...\n",
            "Generating token 6...\n",
            "Generating token 7...\n",
            "Generating token 8...\n",
            "Generating token 9...\n",
            "Generating token 10...\n",
            "Generating token 11...\n",
            "Generating token 12...\n",
            "Generating token 13...\n",
            "Generating token 14...\n",
            "Generating token 15...\n",
            "Generating token 16...\n",
            "Generating token 17...\n",
            "Generating token 18...\n",
            "Generating token 19...\n",
            "Generating token 20...\n",
            "Generating token 21...\n",
            "Generating token 22...\n",
            "Generating token 23...\n",
            "Generating token 24...\n",
            "Generating token 25...\n",
            "Generating token 26...\n",
            "Generating token 27...\n",
            "Generating token 28...\n",
            "Generating token 29...\n",
            "Generating token 30...\n",
            "Generating token 31...\n",
            "Generating token 32...\n",
            "Generating token 33...\n",
            "Generating token 34...\n",
            "Generating token 35...\n",
            "Generating token 36...\n",
            "Generating token 37...\n",
            "Generating token 38...\n",
            "Generating token 39...\n",
            "Generating token 40...\n",
            "Generating token 41...\n",
            "Generating token 42...\n",
            "Generating token 43...\n",
            "Generating token 44...\n",
            "Generating token 45...\n",
            "Generating token 46...\n",
            "Generating token 47...\n",
            "Generating token 48...\n",
            "Generating token 49...\n",
            "Generating token 50...\n",
            "Generating token 51...\n",
            "Generating token 52...\n",
            "Generating token 53...\n",
            "Generating token 54...\n",
            "Generating token 55...\n",
            "Generating token 56...\n",
            "Generating token 57...\n",
            "Generating token 58...\n",
            "Generating token 59...\n",
            "Generating token 60...\n",
            "Generating token 61...\n",
            "Generating token 62...\n",
            "Generating token 63...\n",
            "Generating token 64...\n",
            "Generating token 65...\n",
            "Generating token 66...\n",
            "Generating token 67...\n",
            "Generating token 68...\n",
            "Generating token 69...\n",
            "Generating token 70...\n",
            "Generating token 71...\n",
            "Generating token 72...\n",
            "Generating token 73...\n",
            "Generating token 74...\n",
            "Generating token 75...\n",
            "Generating token 76...\n",
            "Generating token 77...\n",
            "Generating token 78...\n",
            "Generating token 79...\n",
            "Generating token 80...\n",
            "Generating token 81...\n",
            "Generating token 82...\n",
            "Generating token 83...\n",
            "Generating token 84...\n",
            "Generating token 85...\n",
            "Generating token 86...\n",
            "Generating token 87...\n",
            "Generating token 88...\n",
            "Generating token 89...\n",
            "Generating token 90...\n",
            "Generating token 91...\n",
            "Generating token 92...\n",
            "Generating token 93...\n",
            "Generating token 94...\n",
            "Generating token 95...\n",
            "Generating token 96...\n",
            "Generating token 97...\n",
            "Generating token 98...\n",
            "Generating token 99...\n",
            "Generating token 100...\n",
            "Generating token 101...\n",
            "Generating token 102...\n",
            "Generating token 103...\n",
            "Generating token 104...\n",
            "Generating token 105...\n",
            "Generating token 106...\n",
            "Generating token 107...\n",
            "Generating token 108...\n",
            "Generating token 109...\n",
            "Generating token 110...\n",
            "Generating token 111...\n",
            "Generating token 112...\n",
            "Generating token 113...\n",
            "Generating token 114...\n",
            "Generating token 115...\n",
            "Generating token 116...\n",
            "Generating token 117...\n",
            "Generating token 118...\n",
            "Generating token 119...\n",
            "Generating token 120...\n",
            "Generating token 121...\n",
            "Generating token 122...\n",
            "Generating token 123...\n",
            "Generating token 124...\n",
            "Generating token 125...\n",
            "Generating token 126...\n",
            "Generating token 127...\n",
            "Generating token 128...\n",
            "EOS token encountered — stopping generation.\n"
          ]
        }
      ],
      "source": [
        "out = generate(llama, idx, context_len=5024, max_new_tok=1000, top_k=35, temp=0.9)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "id": "f34f3f8c",
      "metadata": {
        "id": "f34f3f8c"
      },
      "outputs": [],
      "source": [
        "out = out.squeeze()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "0641dfae",
      "metadata": {
        "id": "0641dfae"
      },
      "outputs": [],
      "source": [
        "def extract_assistant_output(out, tokenizer):\n",
        "    text = tokenizer.decode(out.tolist())\n",
        "    if \"<|start_header_id|>assistant<|end_header_id|>\" in text:\n",
        "        text = text.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\n",
        "    return text.split(\"<|eot_id|>\")[0].strip()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assistant_response = extract_assistant_output(out, tok)\n",
        "print(assistant_response)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0tXBYLpzVT9",
        "outputId": "3aa443fb-d238-4dce-e3ee-5738c6b21a72"
      },
      "id": "A0tXBYLpzVT9",
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sarthak's eConnect is an educational platform meant for supporting students in preparing for exams such as JEE, NEET, UPSC, and others in India. It provides study resources like notes, sample papers, quizzes, and mock tests to help students prepare for these exams. Users can also engage with the community through a Q&A session, asking and answering questions related to academics and exams. This platform aims to foster collaborative learning, clarify doubts, and promote the sharing of knowledge. Furthermore, it offers daily practice questions, current affairs updates, and in-depth topic discussions to facilitate better exam preparation and a deeper understanding of exam topics.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
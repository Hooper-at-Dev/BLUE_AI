{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23e1f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Tuple, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "from pathlib import Path\n",
    "from typing import (\n",
    "    AbstractSet,\n",
    "    cast,\n",
    "    Collection,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Literal,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.nn.attention import sdpa_kernel, SDPBackend\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f79023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4080e4e0",
   "metadata": {},
   "source": [
    "# RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6725e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angles(theeta, dim, seq_len):\n",
    "    pos = 1/theeta**(torch.arange(0, dim, 2, device=device).float()/dim)\n",
    "    t = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(t, pos)\n",
    "    unit_vecs = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return unit_vecs\n",
    "\n",
    "def brodcast(unit_vecs, x):\n",
    "    # print(f\"unit_vecs.shape: {unit_vecs.shape}\\nX.shape: {x.shape[1], x.shape[-1]}\")\n",
    "    assert unit_vecs.shape == (x.shape[1], x.shape[-1])\n",
    "    n_dim = x.ndim\n",
    "    shape = [d if i == 1 or i == n_dim-1 else 1 for i,d in enumerate(x.shape)]\n",
    "    return unit_vecs.view(*shape)\n",
    "\n",
    "def RoPE(W_Q, W_K, unit_vecs):\n",
    "    complex_W_Q = torch.view_as_complex(W_Q.float().reshape(*W_Q.shape[:-1], -1, 2))\n",
    "    complex_W_K = torch.view_as_complex(W_K.float().reshape(*W_K.shape[:-1], -1, 2))\n",
    "    # print(complex_W_Q.shape)\n",
    "    pos = brodcast(unit_vecs, complex_W_K)\n",
    "    embedded_W_Q = torch.view_as_real(complex_W_Q * pos).float().flatten(3)\n",
    "    embedded_W_K = torch.view_as_real(complex_W_K * pos).float().flatten(3)\n",
    "    return embedded_W_Q, embedded_W_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9f6c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGURATIONS = {\n",
    "  \"DIM\": 3072,\n",
    "  \"FFN_DIM\": 8192,\n",
    "  \"N_LAYERS\": 28,\n",
    "  \"N_HEADS\": 24,\n",
    "  \"N_KV_HEADS\": 8,\n",
    "  \"VOCAB_SIZE\": 128256,\n",
    "  \"NORM_EPS\": 1e-5,\n",
    "  \"ROPE_THETA\": 500000,\n",
    "  \"MAX_BATCH_SIZE\": 4,\n",
    "  \"MAX_SEQ_LEN\": 6000,\n",
    "  \"N_KV_HEAD_REP\": 24 // 8,\n",
    "  \"HEAD_DIM\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d678e25e",
   "metadata": {},
   "source": [
    "# LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "64b01afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, head_dim, n_kv_heads, n_kv_heads_reps, max_batch_size, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Linear(dim, n_heads * head_dim, bias=False)\n",
    "        self.W_K = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n",
    "        self.W_V = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n",
    "\n",
    "        self.register_buffer('CACHE_K', torch.zeros(\n",
    "            (max_batch_size, max_seq_len, n_kv_heads, head_dim))\n",
    "        )\n",
    "        self.register_buffer('CACHE_V', torch.zeros(\n",
    "            (max_batch_size, max_seq_len, n_kv_heads, head_dim))\n",
    "        )\n",
    "\n",
    "        self.wo = nn.Linear(dim, dim)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_kv_heads_reps = n_kv_heads_reps\n",
    "\n",
    "\n",
    "    def forward(self,x, freq=None, start_pos=0, mask=None):\n",
    "        bhz, seq_len, _ = x.shape\n",
    "\n",
    "        query = self.W_Q(x).view(bhz, seq_len, self.n_heads, self.head_dim)\n",
    "        key = self.W_K(x).view(bhz, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        value = self.W_V(x).view(bhz, seq_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        query, key = RoPE(query, key, freq)\n",
    "\n",
    "        self.CACHE_K[:bhz, start_pos:start_pos+seq_len] = key\n",
    "        self.CACHE_V[:bhz, start_pos:start_pos+seq_len] = value\n",
    "\n",
    "        keys = self.CACHE_K[:bhz, :start_pos+seq_len]\n",
    "        values = self.CACHE_V[:bhz, :start_pos+seq_len]\n",
    "\n",
    "        keys = torch.repeat_interleave(input=keys, repeats=self.n_kv_heads_reps, dim=-2)\n",
    "        values = torch.repeat_interleave(input=values, repeats=self.n_kv_heads_reps, dim=-2)\n",
    "\n",
    "        queries = query.transpose(1,2)\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        \n",
    "        out = F.scaled_dot_product_attention(queries, keys, values, attn_mask=mask)\n",
    "        out = out.transpose(1,2).contiguous().view(bhz, seq_len, -1)\n",
    "\n",
    "        return self.wo(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd9fc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, norm_eps):\n",
    "        super().__init__()\n",
    "        self.norm_eps = norm_eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.norm_eps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self._norm(x.float()).type_as(x)\n",
    "        return out * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e8997b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w1 = nn.Linear(dim, ffn_dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, ffn_dim, bias=False)\n",
    "        self.w2 = nn.Linear(ffn_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fd82bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Attention_Norm = RMSNorm(dim=config[\"DIM\"], norm_eps=config[\"NORM_EPS\"])\n",
    "        self.FFN_Norm = RMSNorm(dim=config[\"DIM\"], norm_eps=config[\"NORM_EPS\"])\n",
    "        self.Attention = Attention(dim=config[\"DIM\"],\n",
    "                                   n_heads=config[\"N_HEADS\"],\n",
    "                                   head_dim=config[\"HEAD_DIM\"],\n",
    "                                   n_kv_heads=config[\"N_KV_HEADS\"],\n",
    "                                   n_kv_heads_reps=config[\"N_KV_HEAD_REP\"],\n",
    "                                   max_batch_size=config[\"MAX_BATCH_SIZE\"],\n",
    "                                   max_seq_len=config[\"MAX_SEQ_LEN\"])\n",
    "        self.FeedForward = FeedForward(dim=config[\"DIM\"],\n",
    "                                       ffn_dim=config[\"FFN_DIM\"])\n",
    "    def forward(self, x, freq, start_pos, mask):\n",
    "        shortcut = x\n",
    "        x = self.Attention_Norm(x)\n",
    "        x = self.Attention(x, freq, start_pos, mask)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.FFN_Norm(x)\n",
    "        x = self.FeedForward(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0af5202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLAMA_3(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(config[\"VOCAB_SIZE\"], config[\"DIM\"])\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(config[\"N_LAYERS\"]):\n",
    "            self.layers.append(Transformer_Block(config))\n",
    "        self.norm = RMSNorm(config[\"DIM\"], config[\"NORM_EPS\"])\n",
    "        self.output = nn.Linear(config[\"DIM\"], config[\"VOCAB_SIZE\"], bias=False)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'freq',\n",
    "            calculate_angles(\n",
    "                config[\"ROPE_THETA\"],\n",
    "                config[\"HEAD_DIM\"],\n",
    "                config[\"MAX_SEQ_LEN\"] * 2\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def forward(self, tokens, start_pos):\n",
    "        bhz, seq_len = tokens.shape\n",
    "        x = self.tok_embedding(tokens)\n",
    "        freq = self.freq[start_pos : start_pos+seq_len]\n",
    "\n",
    "        mask = None\n",
    "        if seq_len > 1:\n",
    "            mask = torch.full((seq_len, seq_len), float(\"-inf\"), device=tokens.device) \n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, freq, start_pos, mask )\n",
    "        x = self.norm(x)\n",
    "        x = self.output(x).float()\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83e91f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = LLAMA_3(config=CONFIGURATIONS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "658010aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of parameters: 3,606,838,272\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in llama.parameters())\n",
    "print(f\"Total Number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7bae4694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 13.436519622802734 GB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb/1024} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfc77b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_Text(model, idx, max_tokens, context_size, start_pos):\n",
    "    for _ in range(max_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.inference_mode():\n",
    "            logits = model(idx_cond, start_pos)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_idx = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "            idx = torch.cat((idx, next_idx), dim=-1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c222ee",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2d47063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    no_reserved_special_tokens = 256\n",
    "    pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E501\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        mergeable_ranks = load_tiktoken_bpe(model_path)\n",
    "        num_base_tokens = len(mergeable_ranks)\n",
    "        special_tokens = [\n",
    "            \"<|begin_of_text|>\",\n",
    "            \"<|end_of_text|>\",\n",
    "            \"<|reserved_special_token_0|>\",\n",
    "            \"<|reserved_special_token_1|>\",\n",
    "            \"<|reserved_special_token_2|>\",\n",
    "            \"<|reserved_special_token_3|>\",\n",
    "            \"<|start_header_id|>\",\n",
    "            \"<|end_header_id|>\",\n",
    "            \"<|reserved_special_token_4|>\",\n",
    "            \"<|eot_id|>\",  # end of turn\n",
    "        ] + [f\"<|reserved_special_token_{i}|>\" for i in range(5, self.no_reserved_special_tokens - 5)]\n",
    "        \n",
    "        self.special_tokens = {\n",
    "            token: num_base_tokens + i for i, token in enumerate(special_tokens)\n",
    "        }\n",
    "\n",
    "        self.model = tiktoken.Encoding(\n",
    "            name=Path(model_path).name,\n",
    "            pat_str=self.pat_str,\n",
    "            mergeable_ranks=mergeable_ranks,\n",
    "            special_tokens=self.special_tokens\n",
    "        )\n",
    "        self.n_words = self.model.n_vocab\n",
    "        self.bos_id: int = self.special_tokens[\"<|begin_of_text|>\"]\n",
    "        self.eos_id: int = self.special_tokens[\"<|end_of_text|>\"]\n",
    "        self.pad_id: int = -1\n",
    "        self.stop_tokens = {\n",
    "            self.special_tokens[\"<|end_of_text|>\"],\n",
    "            self.special_tokens[\"<|eot_id|>\"],\n",
    "        }\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        s: str,\n",
    "        *,\n",
    "        bos: bool,\n",
    "        eos: bool,\n",
    "        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\n",
    "        disallowed_special: Union[Literal[\"all\"], Collection[str]] = (),\n",
    "        ):\n",
    "        TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n",
    "        MAX_NO_WHITESPACES_CHARS = 25_000\n",
    "        substrs = (\n",
    "            substr\n",
    "            for i in range(0, len(s), TIKTOKEN_MAX_ENCODE_CHARS)\n",
    "            for substr in self._split_whitespaces_or_nonwhitespaces(\n",
    "                s[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS\n",
    "            )\n",
    "        )\n",
    "        t: List[int] = []\n",
    "        for substr in substrs:\n",
    "            t.extend(\n",
    "                self.model.encode(\n",
    "                    substr,\n",
    "                    allowed_special=allowed_special,\n",
    "                    disallowed_special=disallowed_special,\n",
    "                )\n",
    "            )\n",
    "        if bos:\n",
    "            t.insert(0, self.bos_id)\n",
    "        if eos:\n",
    "            t.append(self.eos_id)\n",
    "        return t\n",
    "    def decode(self, t: Sequence[int]) -> str:\n",
    "        return self.model.decode(cast(List[int], t))\n",
    "\n",
    "    def _split_whitespaces_or_nonwhitespaces(self,\n",
    "        s: str, max_consecutive_slice_len: int\n",
    "    ):\n",
    "        current_slice_len = 0\n",
    "        current_slice_is_space = s[0].isspace() if len(s) > 0 else False\n",
    "        slice_start = 0\n",
    "\n",
    "        for i in range(len(s)):\n",
    "            is_now_space = s[i].isspace()\n",
    "\n",
    "            if current_slice_is_space ^ is_now_space:\n",
    "                current_slice_len = 1\n",
    "                current_slice_is_space = is_now_space\n",
    "            else:\n",
    "                current_slice_len += 1\n",
    "                if current_slice_len > max_consecutive_slice_len:\n",
    "                    yield s[slice_start:i]\n",
    "                    slice_start = i\n",
    "                    current_slice_len = 1\n",
    "        yield s[slice_start:]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3ea995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(\"Weights/3B-instruct//original/tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a195f25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my name is sarthak历史cede.serializer(cloneγωγή'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = tok.encode(s=\"my name is sarthak\", bos=False, eos=False)\n",
    "z = torch.tensor([z], dtype=torch.long, device=device)\n",
    "d = Generate_Text(llama, z, 5, 10, 0)\n",
    "d = d.squeeze(dim=0)\n",
    "tok.decode(d.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c12110fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_gpt_2_355M import load_consolidated_pth_weights, load_llama_weights2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "388dd73d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 255 tensors from ./Weights/3B-instruct/original/consolidated.00.pth\n"
     ]
    }
   ],
   "source": [
    "dir = \"./Weights/3B-instruct/original/consolidated.00.pth\"\n",
    "params = load_consolidated_pth_weights(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c72c12b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok_embeddings.weight', 'layers.0.attention.wq.weight', 'layers.0.attention.wk.weight', 'layers.0.attention.wv.weight', 'layers.0.attention.wo.weight', 'layers.0.feed_forward.w1.weight', 'layers.0.feed_forward.w3.weight', 'layers.0.feed_forward.w2.weight', 'layers.0.attention_norm.weight', 'layers.0.ffn_norm.weight', 'layers.1.attention.wq.weight', 'layers.1.attention.wk.weight', 'layers.1.attention.wv.weight', 'layers.1.attention.wo.weight', 'layers.1.feed_forward.w1.weight', 'layers.1.feed_forward.w3.weight', 'layers.1.feed_forward.w2.weight', 'layers.1.attention_norm.weight', 'layers.1.ffn_norm.weight', 'layers.2.attention.wq.weight', 'layers.2.attention.wk.weight', 'layers.2.attention.wv.weight', 'layers.2.attention.wo.weight', 'layers.2.feed_forward.w1.weight', 'layers.2.feed_forward.w3.weight', 'layers.2.feed_forward.w2.weight', 'layers.2.attention_norm.weight', 'layers.2.ffn_norm.weight', 'layers.3.attention.wq.weight', 'layers.3.attention.wk.weight', 'layers.3.attention.wv.weight', 'layers.3.attention.wo.weight', 'layers.3.feed_forward.w1.weight', 'layers.3.feed_forward.w3.weight', 'layers.3.feed_forward.w2.weight', 'layers.3.attention_norm.weight', 'layers.3.ffn_norm.weight', 'layers.4.attention.wq.weight', 'layers.4.attention.wk.weight', 'layers.4.attention.wv.weight', 'layers.4.attention.wo.weight', 'layers.4.feed_forward.w1.weight', 'layers.4.feed_forward.w3.weight', 'layers.4.feed_forward.w2.weight', 'layers.4.attention_norm.weight', 'layers.4.ffn_norm.weight', 'layers.5.attention.wq.weight', 'layers.5.attention.wk.weight', 'layers.5.attention.wv.weight', 'layers.5.attention.wo.weight', 'layers.5.feed_forward.w1.weight', 'layers.5.feed_forward.w3.weight', 'layers.5.feed_forward.w2.weight', 'layers.5.attention_norm.weight', 'layers.5.ffn_norm.weight', 'layers.6.attention.wq.weight', 'layers.6.attention.wk.weight', 'layers.6.attention.wv.weight', 'layers.6.attention.wo.weight', 'layers.6.feed_forward.w1.weight', 'layers.6.feed_forward.w3.weight', 'layers.6.feed_forward.w2.weight', 'layers.6.attention_norm.weight', 'layers.6.ffn_norm.weight', 'layers.7.attention.wq.weight', 'layers.7.attention.wk.weight', 'layers.7.attention.wv.weight', 'layers.7.attention.wo.weight', 'layers.7.feed_forward.w1.weight', 'layers.7.feed_forward.w3.weight', 'layers.7.feed_forward.w2.weight', 'layers.7.attention_norm.weight', 'layers.7.ffn_norm.weight', 'layers.8.attention.wq.weight', 'layers.8.attention.wk.weight', 'layers.8.attention.wv.weight', 'layers.8.attention.wo.weight', 'layers.8.feed_forward.w1.weight', 'layers.8.feed_forward.w3.weight', 'layers.8.feed_forward.w2.weight', 'layers.8.attention_norm.weight', 'layers.8.ffn_norm.weight', 'layers.9.attention.wq.weight', 'layers.9.attention.wk.weight', 'layers.9.attention.wv.weight', 'layers.9.attention.wo.weight', 'layers.9.feed_forward.w1.weight', 'layers.9.feed_forward.w3.weight', 'layers.9.feed_forward.w2.weight', 'layers.9.attention_norm.weight', 'layers.9.ffn_norm.weight', 'layers.10.attention.wq.weight', 'layers.10.attention.wk.weight', 'layers.10.attention.wv.weight', 'layers.10.attention.wo.weight', 'layers.10.feed_forward.w1.weight', 'layers.10.feed_forward.w3.weight', 'layers.10.feed_forward.w2.weight', 'layers.10.attention_norm.weight', 'layers.10.ffn_norm.weight', 'layers.11.attention.wq.weight', 'layers.11.attention.wk.weight', 'layers.11.attention.wv.weight', 'layers.11.attention.wo.weight', 'layers.11.feed_forward.w1.weight', 'layers.11.feed_forward.w3.weight', 'layers.11.feed_forward.w2.weight', 'layers.11.attention_norm.weight', 'layers.11.ffn_norm.weight', 'layers.12.attention.wq.weight', 'layers.12.attention.wk.weight', 'layers.12.attention.wv.weight', 'layers.12.attention.wo.weight', 'layers.12.feed_forward.w1.weight', 'layers.12.feed_forward.w3.weight', 'layers.12.feed_forward.w2.weight', 'layers.12.attention_norm.weight', 'layers.12.ffn_norm.weight', 'layers.13.attention.wq.weight', 'layers.13.attention.wk.weight', 'layers.13.attention.wv.weight', 'layers.13.attention.wo.weight', 'layers.13.feed_forward.w1.weight', 'layers.13.feed_forward.w3.weight', 'layers.13.feed_forward.w2.weight', 'layers.13.attention_norm.weight', 'layers.13.ffn_norm.weight', 'layers.14.attention.wq.weight', 'layers.14.attention.wk.weight', 'layers.14.attention.wv.weight', 'layers.14.attention.wo.weight', 'layers.14.feed_forward.w1.weight', 'layers.14.feed_forward.w3.weight', 'layers.14.feed_forward.w2.weight', 'layers.14.attention_norm.weight', 'layers.14.ffn_norm.weight', 'layers.15.attention.wq.weight', 'layers.15.attention.wk.weight', 'layers.15.attention.wv.weight', 'layers.15.attention.wo.weight', 'layers.15.feed_forward.w1.weight', 'layers.15.feed_forward.w3.weight', 'layers.15.feed_forward.w2.weight', 'layers.15.attention_norm.weight', 'layers.15.ffn_norm.weight', 'layers.16.attention.wq.weight', 'layers.16.attention.wk.weight', 'layers.16.attention.wv.weight', 'layers.16.attention.wo.weight', 'layers.16.feed_forward.w1.weight', 'layers.16.feed_forward.w3.weight', 'layers.16.feed_forward.w2.weight', 'layers.16.attention_norm.weight', 'layers.16.ffn_norm.weight', 'layers.17.attention.wq.weight', 'layers.17.attention.wk.weight', 'layers.17.attention.wv.weight', 'layers.17.attention.wo.weight', 'layers.17.feed_forward.w1.weight', 'layers.17.feed_forward.w3.weight', 'layers.17.feed_forward.w2.weight', 'layers.17.attention_norm.weight', 'layers.17.ffn_norm.weight', 'layers.18.attention.wq.weight', 'layers.18.attention.wk.weight', 'layers.18.attention.wv.weight', 'layers.18.attention.wo.weight', 'layers.18.feed_forward.w1.weight', 'layers.18.feed_forward.w3.weight', 'layers.18.feed_forward.w2.weight', 'layers.18.attention_norm.weight', 'layers.18.ffn_norm.weight', 'layers.19.attention.wq.weight', 'layers.19.attention.wk.weight', 'layers.19.attention.wv.weight', 'layers.19.attention.wo.weight', 'layers.19.feed_forward.w1.weight', 'layers.19.feed_forward.w3.weight', 'layers.19.feed_forward.w2.weight', 'layers.19.attention_norm.weight', 'layers.19.ffn_norm.weight', 'layers.20.attention.wq.weight', 'layers.20.attention.wk.weight', 'layers.20.attention.wv.weight', 'layers.20.attention.wo.weight', 'layers.20.feed_forward.w1.weight', 'layers.20.feed_forward.w3.weight', 'layers.20.feed_forward.w2.weight', 'layers.20.attention_norm.weight', 'layers.20.ffn_norm.weight', 'layers.21.attention.wq.weight', 'layers.21.attention.wk.weight', 'layers.21.attention.wv.weight', 'layers.21.attention.wo.weight', 'layers.21.feed_forward.w1.weight', 'layers.21.feed_forward.w3.weight', 'layers.21.feed_forward.w2.weight', 'layers.21.attention_norm.weight', 'layers.21.ffn_norm.weight', 'layers.22.attention.wq.weight', 'layers.22.attention.wk.weight', 'layers.22.attention.wv.weight', 'layers.22.attention.wo.weight', 'layers.22.feed_forward.w1.weight', 'layers.22.feed_forward.w3.weight', 'layers.22.feed_forward.w2.weight', 'layers.22.attention_norm.weight', 'layers.22.ffn_norm.weight', 'layers.23.attention.wq.weight', 'layers.23.attention.wk.weight', 'layers.23.attention.wv.weight', 'layers.23.attention.wo.weight', 'layers.23.feed_forward.w1.weight', 'layers.23.feed_forward.w3.weight', 'layers.23.feed_forward.w2.weight', 'layers.23.attention_norm.weight', 'layers.23.ffn_norm.weight', 'layers.24.attention.wq.weight', 'layers.24.attention.wk.weight', 'layers.24.attention.wv.weight', 'layers.24.attention.wo.weight', 'layers.24.feed_forward.w1.weight', 'layers.24.feed_forward.w3.weight', 'layers.24.feed_forward.w2.weight', 'layers.24.attention_norm.weight', 'layers.24.ffn_norm.weight', 'layers.25.attention.wq.weight', 'layers.25.attention.wk.weight', 'layers.25.attention.wv.weight', 'layers.25.attention.wo.weight', 'layers.25.feed_forward.w1.weight', 'layers.25.feed_forward.w3.weight', 'layers.25.feed_forward.w2.weight', 'layers.25.attention_norm.weight', 'layers.25.ffn_norm.weight', 'layers.26.attention.wq.weight', 'layers.26.attention.wk.weight', 'layers.26.attention.wv.weight', 'layers.26.attention.wo.weight', 'layers.26.feed_forward.w1.weight', 'layers.26.feed_forward.w3.weight', 'layers.26.feed_forward.w2.weight', 'layers.26.attention_norm.weight', 'layers.26.ffn_norm.weight', 'layers.27.attention.wq.weight', 'layers.27.attention.wk.weight', 'layers.27.attention.wv.weight', 'layers.27.attention.wo.weight', 'layers.27.feed_forward.w1.weight', 'layers.27.feed_forward.w3.weight', 'layers.27.feed_forward.w2.weight', 'layers.27.attention_norm.weight', 'layers.27.ffn_norm.weight', 'norm.weight', 'output.weight']\n"
     ]
    }
   ],
   "source": [
    "print(list(params.keys())[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "53bbdf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE tok_embedding.weight -3.601663775043562e-05\n",
      "BEFORE layers.0.Attention_Norm.weight 1.0\n",
      "BEFORE layers.0.FFN_Norm.weight 1.0\n",
      "BEFORE layers.0.Attention.W_Q.weight -4.207213805784704e-06\n",
      "BEFORE layers.0.Attention.W_K.weight 8.162949598045088e-06\n",
      "BEFORE layers.0.Attention.W_V.weight -6.162651516206097e-06\n",
      "BEFORE layers.0.Attention.wo.weight -1.2410536101015168e-06\n",
      "BEFORE layers.0.Attention.wo.bias -0.0001562635152367875\n",
      "BEFORE layers.0.FeedForward.w1.weight 9.783564109966392e-07\n",
      "BEFORE layers.0.FeedForward.w3.weight -9.322870937467087e-07\n"
     ]
    }
   ],
   "source": [
    "for n, p in list(llama.named_parameters())[:10]:\n",
    "    print(\"BEFORE\", n, p.mean().item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1750c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def weight_injector(my_model, params):\n",
    "    \"\"\"\n",
    "    Inject weights from a LLaMA-like consolidated checkpoint (params)\n",
    "    into a model whose naming differs from the original LLaMA structure.\n",
    "    \"\"\"\n",
    "\n",
    "    device = next(my_model.parameters()).device\n",
    "    dtype = next(my_model.parameters()).dtype\n",
    "\n",
    "    # --- Embeddings ---\n",
    "    my_model.tok_embedding.weight.data.copy_(\n",
    "        params[\"tok_embeddings.weight\"].to(device=device, dtype=dtype)\n",
    "    )\n",
    "\n",
    "    # --- Layers ---\n",
    "    num_layers = len(my_model.layers)\n",
    "    for i in range(num_layers):\n",
    "        layer = my_model.layers[i]\n",
    "\n",
    "        # Attention\n",
    "        layer.Attention.W_Q.weight.data.copy_(\n",
    "            params[f\"layers.{i}.attention.wq.weight\"].to(device=device, dtype=dtype)\n",
    "        )\n",
    "        layer.Attention.W_K.weight.data.copy_(\n",
    "            params[f\"layers.{i}.attention.wk.weight\"].to(device=device, dtype=dtype)\n",
    "        )\n",
    "        layer.Attention.W_V.weight.data.copy_(\n",
    "            params[f\"layers.{i}.attention.wv.weight\"].to(device=device, dtype=dtype)\n",
    "        )\n",
    "        layer.Attention.wo.weight.data.copy_(\n",
    "            params[f\"layers.{i}.attention.wo.weight\"].to(device=device, dtype=dtype)\n",
    "        )\n",
    "\n",
    "        # Optional bias (only if present)\n",
    "        if hasattr(layer.Attention.wo, \"bias\") and f\"layers.{i}.attention.wo.bias\" in params:\n",
    "            layer.Attention.wo.bias.data.copy_(\n",
    "                params[f\"layers.{i}.attention.wo.bias\"].to(device=device, dtype=dtype)\n",
    "            )\n",
    "\n",
    "        # FeedForward\n",
    "        layer.FeedForward.w1.weight.data.copy_(\n",
    "            params[f\"layers.{i}.feed_forward.w1.weight\"].to(device=device, dtype=dtype)\n",
    "        )\n",
    "        layer.FeedForward.w3.weight.data.copy_(\n",
    "            params[f\"layers.{i}.feed_forward.w3.weight\"].to(device=device, dtype=dtype)\n",
    "        )\n",
    "        layer.FeedForward.w2.weight.data.copy_(\n",
    "            params[f\"layers.{i}.feed_forward.w2.weight\"].to(device=device, dtype=dtype)\n",
    "        )\n",
    "\n",
    "        # Norms\n",
    "        layer.Attention_Norm.weight.data.copy_(\n",
    "            params[f\"layers.{i}.attention_norm.weight\"].to(device=device, dtype=dtype)\n",
    "        )\n",
    "        layer.FFN_Norm.weight.data.copy_(\n",
    "            params[f\"layers.{i}.ffn_norm.weight\"].to(device=device, dtype=dtype)\n",
    "        )\n",
    "\n",
    "    # --- Final Norm ---\n",
    "    my_model.norm.weight.data.copy_(\n",
    "        params[\"norm.weight\"].to(device=device, dtype=dtype)\n",
    "    )\n",
    "\n",
    "    # --- Output projection ---\n",
    "    out_w = params[\"output.weight\"]\n",
    "    if my_model.output.weight.shape == out_w.shape:\n",
    "        my_model.output.weight.data.copy_(out_w.to(device=device, dtype=dtype))\n",
    "    elif my_model.output.weight.shape[::-1] == out_w.shape:\n",
    "        my_model.output.weight.data.copy_(out_w.T.to(device=device, dtype=dtype))\n",
    "    else:\n",
    "        raise ValueError(f\"Output weight shape mismatch: model={my_model.output.weight.shape}, ckpt={out_w.shape}\")\n",
    "\n",
    "    print(\"✅ All weights successfully injected into your model!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f327b0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All weights successfully injected into your model!\n"
     ]
    }
   ],
   "source": [
    "weight_injector(llama, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "619e5790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, idx, context_len, max_new_tok, top_k, temp=0.1, eos_id=None):\n",
    "    model.eval()\n",
    "    \n",
    "    for tok_no in range(max_new_tok):\n",
    "        if tok_no == 0:\n",
    "            idx_cond = idx[:, -context_len:] if idx.shape[1] > context_len else idx\n",
    "            start_pos = 0\n",
    "        else:\n",
    "            idx_cond = idx[:, -1:]\n",
    "            start_pos = idx.shape[1] - 1\n",
    "        \n",
    "        print(f\"\\rToken {tok_no}: seq_len={idx_cond.shape[1]}, start_pos={start_pos}\", end=\"\")\n",
    "        \n",
    "        with torch.inference_mode():\n",
    "            logits = model(idx_cond, start_pos)\n",
    "        \n",
    "        logits = logits[:, -1, :]\n",
    "        \n",
    "        if top_k > 0:\n",
    "            top_k_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_k_logits[:, -1:]\n",
    "            logits = torch.where(\n",
    "                logits < min_val,\n",
    "                torch.tensor(float(\"-inf\"), device=logits.device),\n",
    "                logits\n",
    "            )\n",
    "        \n",
    "        if temp > 0.0:\n",
    "            logits = logits / temp\n",
    "        \n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        preds = torch.multinomial(probs, 1)\n",
    "        \n",
    "        if eos_id is not None and preds.item() == eos_id:\n",
    "            print(f\"EOS token reached at {tok_no}\")\n",
    "            break\n",
    "        \n",
    "        idx = torch.cat([idx, preds], dim=1)\n",
    "    \n",
    "    return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7910ccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_and_tokenize(system_prompt, user_prompt, context=None, device=\"cuda\"):\n",
    "    if context:\n",
    "        user_prompt = f\"Use the following context to answer:\\n{context.strip()}\\n\\nQuestion: {user_prompt.strip()}\"\n",
    "\n",
    "    prompt = (\n",
    "        \"<|start_header_id|>system<|end_header_id|>\\n\"\n",
    "        f\"{system_prompt.strip()}\\n\"\n",
    "        \"<|eot_id|>\\n\"\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        f\"{user_prompt.strip()}\\n\"\n",
    "        \"<|eot_id|>\\n\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    )\n",
    "\n",
    "    input_ids = tok.encode(prompt, bos=True, eos=False)\n",
    "    return torch.tensor([input_ids], device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "929da2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a concise and factual AI assistant. When context is provided, summarize it in your own words rather than copying.\"\n",
    "user_prompt = \"What is sarthaks eConnect?\"\n",
    "context = \"Sarthak’s eConnect is an educational platform designed to support students in preparing for competitive and academic exams in India. It provides a wide range of study resources, including notes, sample papers, quizzes, mock tests, and previous year question papers for exams like JEE, NEET, UPSC, SSC, and state boards.The platform also features an active Q&A community, where students, teachers, and experts can interact—asking and answering academic or exam-related questions. It encourages collaborative learning, doubt-solving, and knowledge sharing. Additionally, users can access daily practice questions, current affairs updates, and topic-wise discussions to enhance their preparation and conceptual understanding.\"\n",
    "idx = format_and_tokenize(system_prompt, user_prompt, context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d5d7ab00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 233])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6c1d2eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DEVICE DIAGNOSTICS:\n",
      "Model weights device: cuda:0\n",
      "Layer 0 KV cache K: cuda:0\n",
      "Layer 0 KV cache V: cuda:0\n",
      "RoPE freqs device: cuda:0\n",
      "Input tokens device: cuda:0\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DEVICE DIAGNOSTICS:\")\n",
    "print(f\"Model weights device: {next(llama.parameters()).device}\")\n",
    "print(f\"Layer 0 KV cache K: {llama.layers[0].Attention.CACHE_K.device}\")\n",
    "print(f\"Layer 0 KV cache V: {llama.layers[0].Attention.CACHE_V.device}\")\n",
    "print(f\"RoPE freqs device: {llama.freq.device}\")\n",
    "print(f\"Input tokens device: {idx.device}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4dda9616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 49: seq_len=1, start_pos=281"
     ]
    }
   ],
   "source": [
    "out = generate(llama, idx, context_len=2048, max_new_tok=50, top_k=10, temp=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8876cfef",
   "metadata": {},
   "source": [
    "# Decoding Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0641dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_assistant_output(out, tokenizer):\n",
    "    out = out.squeeze()\n",
    "    text = tokenizer.decode(out.tolist())\n",
    "    if \"<|start_header_id|>assistant<|end_header_id|>\" in text:\n",
    "        text = text.split(\"<|start_header_id|>assistant<|end_header_id|>\")[-1]\n",
    "    return text.split(\"<|eot_id|>\")[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "57067520",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sarthak's eConnect is an educational platform designed to support students in preparing for competitive and academic exams in India. It provides a wide range of study resources, including notes, sample papers, quizzes, mock tests, and previous year question papers for\n"
     ]
    }
   ],
   "source": [
    "assistant_response = extract_assistant_output(out, tok)\n",
    "print(assistant_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

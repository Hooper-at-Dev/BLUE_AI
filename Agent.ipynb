{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e1f3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not detect ROCm GPU architecture: module 'torch' has no attribute 'version'\n",
      "\n",
      "ROCm GPU architecture detection failed despite ROCm being available.\n",
      "                \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch' has no attribute 'version'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mattention\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sdpa_kernel, SDPBackend\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Linear8bitLt\n",
      "File \u001b[1;32md:\\Conda\\envs\\pyTorch\\lib\\site-packages\\bitsandbytes\\__init__.py:12\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _ops, research, utils\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     14\u001b[0m     MatmulLtState,\n\u001b[0;32m     15\u001b[0m     matmul,\n\u001b[0;32m     16\u001b[0m     matmul_4bit,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpu\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops \u001b[38;5;28;01mas\u001b[39;00m cpu_ops\n",
      "File \u001b[1;32md:\\Conda\\envs\\pyTorch\\lib\\site-packages\\bitsandbytes\\research\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      3\u001b[0m     matmul_fp8_global,\n\u001b[0;32m      4\u001b[0m     matmul_fp8_mixed,\n\u001b[0;32m      5\u001b[0m     switchback_bnb,\n\u001b[0;32m      6\u001b[0m )\n",
      "File \u001b[1;32md:\\Conda\\envs\\pyTorch\\lib\\site-packages\\bitsandbytes\\research\\autograd\\_functions.py:8\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalOutlierPooler, MatmulLtState\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# math.prod not compatible with python < 3.8\u001b[39;00m\n",
      "File \u001b[1;32md:\\Conda\\envs\\pyTorch\\lib\\site-packages\\bitsandbytes\\autograd\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_inverse_transform_indices, undo_layout\n",
      "File \u001b[1;32md:\\Conda\\envs\\pyTorch\\lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# The inverse transformation for the colTuring and colAmpere format were contributed by Alex Borzunov:\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# https://github.com/bigscience-workshop/petals/blob/main/src/petals/utils/linear8bitlt_patch.py\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124;03m    This class pools outlier dimensions across layers.\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124;03m    This is particularly important for small models where outlier features\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    are less systematic and occur with low frequency.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\Conda\\envs\\pyTorch\\lib\\site-packages\\bitsandbytes\\functional.py:18\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pack_dict_to_tensor, unpack_tensor_to_dict\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HIP_ENVIRONMENT, lib\n\u001b[0;32m     20\u001b[0m name2qmap \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     22\u001b[0m \u001b[38;5;124;03m\"\"\"C FUNCTIONS FOR OPTIMIZERS\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\Conda\\envs\\pyTorch\\lib\\site-packages\\bitsandbytes\\cextension.py:304\u001b[0m\n\u001b[0;32m    302\u001b[0m HIP_ENVIRONMENT \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    303\u001b[0m BNB_BACKEND \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCPU\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[38;5;241m.\u001b[39mhip:\n\u001b[0;32m    305\u001b[0m     HIP_ENVIRONMENT \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    306\u001b[0m     BNB_BACKEND \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROCm\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32md:\\Conda\\envs\\pyTorch\\lib\\site-packages\\torch\\__init__.py:2688\u001b[0m, in \u001b[0;36m__getattr__\u001b[1;34m(name)\u001b[0m\n\u001b[0;32m   2685\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m _lazy_modules:\n\u001b[0;32m   2686\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m-> 2688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch' has no attribute 'version'"
     ]
    }
   ],
   "source": [
    "from typing import Optional, Tuple, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "from pathlib import Path\n",
    "from typing import (\n",
    "    AbstractSet,\n",
    "    cast,\n",
    "    Collection,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Literal,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n",
    "import json\n",
    "import numpy as np\n",
    "from torch.nn.attention import sdpa_kernel, SDPBackend\n",
    "import time\n",
    "from bitsandbytes.nn import Linear8bitLt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f79023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4080e4e0",
   "metadata": {},
   "source": [
    "# RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6725e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angles(theeta, dim, seq_len):\n",
    "    pos = 1/theeta**(torch.arange(0, dim, 2, device=device).float()/dim)\n",
    "    t = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(t, pos)\n",
    "    unit_vecs = torch.polar(torch.ones_like(freqs), freqs).to(device)\n",
    "    return unit_vecs\n",
    "\n",
    "def brodcast(unit_vecs, x):\n",
    "    # print(f\"unit_vecs.shape: {unit_vecs.shape}\\nX.shape: {x.shape[1], x.shape[-1]}\")\n",
    "    assert unit_vecs.shape == (x.shape[1], x.shape[-1])\n",
    "    n_dim = x.ndim\n",
    "    shape = [d if i == 1 or i == n_dim-1 else 1 for i,d in enumerate(x.shape)]\n",
    "    return unit_vecs.view(*shape)\n",
    "\n",
    "def RoPE(W_Q, W_K, unit_vecs):\n",
    "    complex_W_Q = torch.view_as_complex(W_Q.float().reshape(*W_Q.shape[:-1], -1, 2)).to(device)\n",
    "    complex_W_K = torch.view_as_complex(W_K.float().reshape(*W_K.shape[:-1], -1, 2)).to(device)\n",
    "    # print(complex_W_Q.shape)\n",
    "    pos = brodcast(unit_vecs, complex_W_K).to(device)\n",
    "    embedded_W_Q = torch.view_as_real(complex_W_Q * pos).float().flatten(3)\n",
    "    embedded_W_K = torch.view_as_real(complex_W_K * pos).float().flatten(3)\n",
    "    return embedded_W_Q, embedded_W_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b08c6dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 6, 2, 2])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W_Q = torch.rand((1,6,2,4))\n",
    "complex_W_Q = torch.view_as_complex(W_Q.float().reshape(*W_Q.shape[:-1], -1, 2))\n",
    "complex_W_Q.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9f6c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGURATIONS = {\n",
    "    \"DIM\": 3072,\n",
    "    \"FFN_DIM\": 8192,\n",
    "    \"N_LAYERS\": 28,\n",
    "    \"N_HEADS\": 24,\n",
    "    \"N_KV_HEADS\": 8,\n",
    "    \"VOCAB_SIZE\": 128256,\n",
    "    \"NORM_EPS\": 1e-5,\n",
    "    \"ROPE_THETA\": 500000,\n",
    "    \"MAX_BATCH_SIZE\": 4,\n",
    "    \"MAX_SEQ_LEN\": 128,\n",
    "    \"N_KV_HEAD_REP\": 24 // 8,\n",
    "    \"HEAD_DIM\": 3072 // 24\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d678e25e",
   "metadata": {},
   "source": [
    "# LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b01afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, head_dim, n_kv_heads, n_kv_heads_reps, max_batch_size, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Linear(dim, n_heads * head_dim, bias=False)\n",
    "        self.W_K = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n",
    "        self.W_V = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n",
    "\n",
    "        self.CACHE_K = torch.zeros(\n",
    "            (max_batch_size, max_seq_len, n_kv_heads, head_dim),\n",
    "            device=device\n",
    "        )\n",
    "        self.CACHE_V = torch.zeros(\n",
    "            (max_batch_size, max_seq_len, n_kv_heads, head_dim),\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        self.wo = nn.Linear(dim, dim)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_kv_heads_reps = n_kv_heads_reps\n",
    "\n",
    "\n",
    "    def forward(self,x, freq=None, start_pos=0, mask=None):\n",
    "        bhz, seq_len, _ = x.shape\n",
    "\n",
    "        query = self.W_Q(x).view(bhz, seq_len, self.n_heads, self.head_dim)\n",
    "        key = self.W_K(x).view(bhz, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        value = self.W_V(x).view(bhz, seq_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        query, key = RoPE(query, key, freq)\n",
    "\n",
    "        # self.CACHE_K.to(device)\n",
    "        # self.CACHE_V.to(device)\n",
    "\n",
    "        self.CACHE_K[:bhz, start_pos:start_pos+seq_len] = key\n",
    "        self.CACHE_V[:bhz, start_pos:start_pos+seq_len] = value\n",
    "\n",
    "        keys = self.CACHE_K[:bhz, :start_pos+seq_len]\n",
    "        values = self.CACHE_V[:bhz, :start_pos+seq_len]\n",
    "\n",
    "        keys = torch.repeat_interleave(input=keys, repeats=self.n_kv_heads_reps, dim=-2)\n",
    "        values = torch.repeat_interleave(input=values, repeats=self.n_kv_heads_reps, dim=-2)\n",
    "\n",
    "        queries = query.transpose(1,2)\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        \n",
    "        out = F.scaled_dot_product_attention(queries, keys, values, attn_mask=mask)\n",
    "        out = out.transpose(1,2).contiguous().view(bhz, seq_len, -1)\n",
    "\n",
    "        return self.wo(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd9fc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, norm_eps):\n",
    "        super().__init__()\n",
    "        self.norm_eps = norm_eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.norm_eps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self._norm(x.float()).type_as(x)\n",
    "        return out * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e8997b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w1 = nn.Linear(dim, ffn_dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, ffn_dim, bias=False)\n",
    "        self.w2 = nn.Linear(ffn_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fd82bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Attention_Norm = RMSNorm(dim=config[\"DIM\"], norm_eps=config[\"NORM_EPS\"])\n",
    "        self.FFN_Norm = RMSNorm(dim=config[\"DIM\"], norm_eps=config[\"NORM_EPS\"])\n",
    "        self.Attention = Attention(dim=config[\"DIM\"],\n",
    "                                   n_heads=config[\"N_HEADS\"],\n",
    "                                   head_dim=config[\"HEAD_DIM\"],\n",
    "                                   n_kv_heads=config[\"N_KV_HEADS\"],\n",
    "                                   n_kv_heads_reps=config[\"N_KV_HEAD_REP\"],\n",
    "                                   max_batch_size=config[\"MAX_BATCH_SIZE\"],\n",
    "                                   max_seq_len=config[\"MAX_SEQ_LEN\"])\n",
    "        self.FeedForward = FeedForward(dim=config[\"DIM\"],\n",
    "                                       ffn_dim=config[\"FFN_DIM\"])\n",
    "    def forward(self, x, freq, start_pos, mask):\n",
    "        shortcut = x\n",
    "        x = self.Attention_Norm(x)\n",
    "        x = self.Attention(x, freq, start_pos, mask)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.FFN_Norm(x)\n",
    "        x = self.FeedForward(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0af5202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLAMA_3(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(config[\"VOCAB_SIZE\"], config[\"DIM\"])\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(config[\"N_LAYERS\"]):\n",
    "            self.layers.append(Transformer_Block(config))\n",
    "        self.norm = RMSNorm(config[\"DIM\"], config[\"NORM_EPS\"])\n",
    "        self.output = nn.Linear(config[\"DIM\"], config[\"VOCAB_SIZE\"], bias=False)\n",
    "\n",
    "        self.freq = calculate_angles(\n",
    "            config[\"ROPE_THETA\"],\n",
    "            config[\"HEAD_DIM\"],\n",
    "            config[\"MAX_SEQ_LEN\"] * 2\n",
    "        )\n",
    "    \n",
    "    def forward(self, tokens, start_pos):\n",
    "        bhz, seq_len = tokens.shape\n",
    "        x = self.tok_embedding(tokens)\n",
    "        freq = self.freq[start_pos : start_pos+seq_len]\n",
    "\n",
    "        mask = None\n",
    "        if seq_len > 1:\n",
    "            mask = torch.full((seq_len, seq_len), float(\"-inf\"), device=tokens.device) \n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, freq, start_pos, mask )\n",
    "        x = self.norm(x)\n",
    "        x = self.output(x).float()\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83e91f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = LLAMA_3(config=CONFIGURATIONS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "658010aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of parameters: 3,606,838,272\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in llama.parameters())\n",
    "print(f\"Total Number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7bae4694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size of the model: 13.436519622802734 GB\n"
     ]
    }
   ],
   "source": [
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb/1024} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfc77b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_Text(model, idx, max_tokens, context_size, start_pos):\n",
    "    for _ in range(max_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.inference_mode():\n",
    "            logits = model(idx_cond, start_pos)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_idx = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "            idx = torch.cat((idx, next_idx), dim=-1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c222ee",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d47063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    no_reserved_special_tokens = 256\n",
    "    pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E501\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        mergeable_ranks = load_tiktoken_bpe(\"./Weights/Llama-3.2-3B/original/tokenizer.model\")\n",
    "        num_base_tokens = len(mergeable_ranks)\n",
    "        special_tokens = [\n",
    "            \"<|begin_of_text|>\",\n",
    "            \"<|end_of_text|>\",\n",
    "            \"<|reserved_special_token_0|>\",\n",
    "            \"<|reserved_special_token_1|>\",\n",
    "            \"<|reserved_special_token_2|>\",\n",
    "            \"<|reserved_special_token_3|>\",\n",
    "            \"<|start_header_id|>\",\n",
    "            \"<|end_header_id|>\",\n",
    "            \"<|reserved_special_token_4|>\",\n",
    "            \"<|eot_id|>\",  # end of turn\n",
    "        ] + [f\"<|reserved_special_token_{i}|>\" for i in range(5, self.no_reserved_special_tokens - 5)]\n",
    "        \n",
    "        self.special_tokens = {\n",
    "            token: num_base_tokens + i for i, token in enumerate(special_tokens)\n",
    "        }\n",
    "\n",
    "        self.model = tiktoken.Encoding(\n",
    "            name=Path(model_path).name,\n",
    "            pat_str=self.pat_str,\n",
    "            mergeable_ranks=mergeable_ranks,\n",
    "            special_tokens=self.special_tokens\n",
    "        )\n",
    "        self.n_words = self.model.n_vocab\n",
    "        self.bos_id: int = self.special_tokens[\"<|begin_of_text|>\"]\n",
    "        self.eos_id: int = self.special_tokens[\"<|end_of_text|>\"]\n",
    "        self.pad_id: int = -1\n",
    "        self.stop_tokens = {\n",
    "            self.special_tokens[\"<|end_of_text|>\"],\n",
    "            self.special_tokens[\"<|eot_id|>\"],\n",
    "        }\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        s: str,\n",
    "        *,\n",
    "        bos: bool,\n",
    "        eos: bool,\n",
    "        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\n",
    "        disallowed_special: Union[Literal[\"all\"], Collection[str]] = (),\n",
    "        ):\n",
    "        TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n",
    "        MAX_NO_WHITESPACES_CHARS = 25_000\n",
    "        substrs = (\n",
    "            substr\n",
    "            for i in range(0, len(s), TIKTOKEN_MAX_ENCODE_CHARS)\n",
    "            for substr in self._split_whitespaces_or_nonwhitespaces(\n",
    "                s[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS\n",
    "            )\n",
    "        )\n",
    "        t: List[int] = []\n",
    "        for substr in substrs:\n",
    "            t.extend(\n",
    "                self.model.encode(\n",
    "                    substr,\n",
    "                    allowed_special=allowed_special,\n",
    "                    disallowed_special=disallowed_special,\n",
    "                )\n",
    "            )\n",
    "        if bos:\n",
    "            t.insert(0, self.bos_id)\n",
    "        if eos:\n",
    "            t.append(self.eos_id)\n",
    "        return t\n",
    "    def decode(self, t: Sequence[int]) -> str:\n",
    "        return self.model.decode(cast(List[int], t))\n",
    "\n",
    "    def _split_whitespaces_or_nonwhitespaces(self,\n",
    "        s: str, max_consecutive_slice_len: int\n",
    "    ):\n",
    "        current_slice_len = 0\n",
    "        current_slice_is_space = s[0].isspace() if len(s) > 0 else False\n",
    "        slice_start = 0\n",
    "\n",
    "        for i in range(len(s)):\n",
    "            is_now_space = s[i].isspace()\n",
    "\n",
    "            if current_slice_is_space ^ is_now_space:\n",
    "                current_slice_len = 1\n",
    "                current_slice_is_space = is_now_space\n",
    "            else:\n",
    "                current_slice_len += 1\n",
    "                if current_slice_len > max_consecutive_slice_len:\n",
    "                    yield s[slice_start:i]\n",
    "                    slice_start = i\n",
    "                    current_slice_len = 1\n",
    "        yield s[slice_start:]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3ea995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(\"Weights/Llama-3.2-3B/original/tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a195f25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Time: 4.8732240200042725\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "z = tok.encode(s=\"my name is sarthak\", bos=False, eos=False)\n",
    "z = torch.tensor([z], dtype=torch.long, device=\"cuda\")\n",
    "d = Generate_Text(llama, z, 5, 10, 0)\n",
    "d = d.squeeze(dim=0)\n",
    "tok.decode(d.tolist())\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Total Time: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12110fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_gpt_2_355M import load_llama3_weights_and_settings, nested_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388dd73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"./Weights/Llama-3.2-3B/original/\"\n",
    "settings, params = load_llama3_weights_and_settings(dir=dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c68746",
   "metadata": {},
   "outputs": [],
   "source": [
    "settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1750c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_injector(llama, params):\n",
    "    device = next(llama.parameters()).device\n",
    "    dtype = next(llama.parameters()).dtype    \n",
    "\n",
    "    llama.tok_embedding.weight.data.copy_(\n",
    "        torch.from_numpy(params[\"tok_embeddings\"][\"weight\"]).to(device=device, dtype=dtype)\n",
    "    )\n",
    "\n",
    "    for b in range(len(params[\"layers\"])):\n",
    "        llama.layers[b].Attention.W_Q.weight.data.copy_(\n",
    "            torch.from_numpy(params[\"layers\"][b][\"attention\"][\"wq\"][\"weight\"]).to(device=device, dtype=dtype)\n",
    "        )\n",
    "        llama.layers[b].Attention.W_K.weight.data.copy_(\n",
    "            torch.from_numpy(params[\"layers\"][b][\"attention\"][\"wk\"][\"weight\"]).to(device=device, dtype=dtype)\n",
    "        )\n",
    "        llama.layers[b].Attention.W_V.weight.data.copy_(\n",
    "            torch.from_numpy(params[\"layers\"][b][\"attention\"][\"wv\"][\"weight\"]).to(device=device, dtype=dtype)\n",
    "        )\n",
    "        llama.layers[b].Attention.wo.weight.data.copy_(\n",
    "            torch.from_numpy(params[\"layers\"][b][\"attention\"][\"wo\"][\"weight\"]).to(device=device, dtype=dtype)\n",
    "        )\n",
    "\n",
    "        llama.layers[b].FeedForward.w1.weight.data.copy_(\n",
    "            torch.from_numpy(params[\"layers\"][b][\"feed_forward\"][\"w1\"][\"weight\"]).to(device=device, dtype=dtype)\n",
    "        )\n",
    "        llama.layers[b].FeedForward.w3.weight.data.copy_(\n",
    "            torch.from_numpy(params[\"layers\"][b][\"feed_forward\"][\"w3\"][\"weight\"]).to(device=device, dtype=dtype)\n",
    "        )\n",
    "        llama.layers[b].FeedForward.w2.weight.data.copy_(\n",
    "            torch.from_numpy(params[\"layers\"][b][\"feed_forward\"][\"w2\"][\"weight\"]).to(device=device, dtype=dtype)\n",
    "        )\n",
    "\n",
    "        llama.layers[b].Attention_Norm.weight.data.copy_(\n",
    "            torch.from_numpy(params[\"layers\"][b][\"attention_norm\"][\"weight\"]).to(device=device, dtype=dtype)\n",
    "        )\n",
    "        llama.layers[b].FFN_Norm.weight.data.copy_(\n",
    "            torch.from_numpy(params[\"layers\"][b][\"ffn_norm\"][\"weight\"]).to(device=device, dtype=dtype)\n",
    "        )\n",
    "\n",
    "    llama.norm.weight.data.copy_(\n",
    "        torch.from_numpy(params[\"norm\"][\"weight\"]).to(device=device, dtype=dtype)\n",
    "    )\n",
    "    llama.output.weight.data.copy_(\n",
    "        torch.from_numpy(params[\"output\"][\"weight\"]).to(device=device, dtype=dtype)\n",
    "    )\n",
    "\n",
    "    print(\"Weights have been loaded successfully......\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_injector(llama, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae3e0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tok.encode(s=\"Whats i saw there was\", bos=False, eos=False)\n",
    "z = torch.tensor([z], dtype=torch.long, device=\"cuda\")\n",
    "d = Generate_Text(llama, z, 20, 10, 0)\n",
    "d = d.squeeze(dim=0)\n",
    "tok.decode(d.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b760b",
   "metadata": {},
   "source": [
    "# DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a59f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = \"./DATA/the-verdict.txt\"\n",
    "\n",
    "with open(DIR, \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cfa930",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef30ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_sample(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
    "                       eval_freq, eval_iter, start_context, tokenizer):\n",
    "    train_losses, val_losses, track_token_seen = [], [], []\n",
    "\n",
    "    token_seen, global_step = 0, -1\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        batch_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
    "        for input_batch, target_batch in batch_iter:\n",
    "            optimizer.zero_grad()\n",
    "            loss = corss_entropy_loss(input_batch, target_batch, device, model)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            token_seen += input_batch.numel()\n",
    "            global_step +=1\n",
    "\n",
    "            if global_step % eval_freq == 0:\n",
    "                train_loss, val_loss = evaluate_model(model, train_loader, val_loader, device, eval_iter)\n",
    "                train_losses.append(train_loss)\n",
    "                val_losses.append(val_loss)\n",
    "                track_token_seen.append(token_seen)\n",
    "                print(f\"Ep {epoch+1}(Step {global_step:06d}): \"\n",
    "                      f\"Train Loss: {train_loss:.3f}, val Loss: {val_loss:.3f}\")\n",
    "                \n",
    "        generate_and_print_sample(model, tokenizer, device, start_context)\n",
    "\n",
    "    return train_losses, val_losses, track_token_seen\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

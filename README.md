# üî∑ BLUE AI

[![Python](https://img.shields.io/badge/Python-3.10+-yellow.svg)](https://www.python.org/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.7.1%2Bcu128-orange.svg)](https://pytorch.org/)
[![Model: LLaMA3](https://img.shields.io/badge/Model-LLAMA3-informational.svg)](https://ai.meta.com/llama/)
[![RAG Enabled](https://img.shields.io/badge/RAG-Enabled-success.svg)](#retrieval-augmented-generation)

> A scalable, modular reimplementation of the **LLaMA 3** family ‚Äî from 1B to 405B parameters ‚Äî with optional **web-based RAG** (Retrieval-Augmented Generation).

---

![BLUE AI Logo](https://drive.google.com/uc?export=view&id=1IimXodUoA3-dwGEaYjCuw8hYzl8Q3A37)


## üöÄ Overview

**BLUE AI** is a next-generation large language model, designed for scalability and adaptability across various model sizes of **LLAMA3**.

At its core, BLUE is a **reimplementation of Meta‚Äôs LLaMA 3 family** of text models ‚Äî fully modular and designed to **scale effortlessly** from lightweight inference on consumer hardware to multi-GPU inference with large parameter sets.

BLUE also integrates a **web-based RAG system**, enabling it to **search the internet in real time**, retrieve relevant context, and use that knowledge to craft accurate and up-to-date responses.

---

## üß† Key Features

### üí° Scalable Model Architecture
- Choose from **1B ‚Üí 405B** parameter configurations.
- Switch between models with ease.
- Built with **efficiency, extensibility**, and **hardware flexibility** in mind.

### üåê Web-Based RAG (Retrieval-Augmented Generation)
- Optional RAG mode allows BLUE to:
  1. Query the web for real-time data.  
  2. Retrieve relevant info.  
  3. Integrate retrieved knowledge into its response generation.
- Keeps BLUE‚Äôs knowledge base **fresh and contextually aware**.

### üß© Configurable and Extensible - Default model: llama-3.2-3B-instruct - Swap configurations and weights easily:
```bash
 CONFIGURATIONS = { "DIM": 3072,
                    "FFN_DIM": 8192,
                    "N_LAYERS": 28,
                    "N_HEADS": 24,
                    "N_KV_HEADS": 8,
                    "VOCAB_SIZE": 128256,              # To change the model, just update the values with the desired models values.
                    "NORM_EPS": 1e-5,
                    "ROPE_THETA": 500000,
                    "MAX_BATCH_SIZE": 2,
                    "MAX_SEQ_LEN": 10000,
                    "N_KV_HEAD_REP": 24 // 8,
                    "HEAD_DIM": 128 }

tok_DIR = "Weights/3B-instruct//original/tokenizer.model"    # Change the path with the actual path of the desired models downloaded weights.
weight_DIR = "./Weights/3B-instruct/original/consolidated.00.pth"

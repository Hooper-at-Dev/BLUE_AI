{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e1f3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "from pathlib import Path\n",
    "from typing import (\n",
    "    AbstractSet,\n",
    "    cast,\n",
    "    Collection,\n",
    "    Dict,\n",
    "    Iterator,\n",
    "    List,\n",
    "    Literal,\n",
    "    Sequence,\n",
    "    TypedDict,\n",
    "    Union,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f79023f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4080e4e0",
   "metadata": {},
   "source": [
    "# RoPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6725e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angles(theeta, dim, seq_len):\n",
    "    pos = 1/theeta**(torch.arange(0, dim, 2, device=device).float()/dim)\n",
    "    t = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(t, pos)\n",
    "    unit_vecs = torch.polar(torch.ones_like(freqs), freqs)\n",
    "    return unit_vecs\n",
    "\n",
    "def brodcast(unit_vecs, x):\n",
    "    # print(f\"unit_vecs.shape: {unit_vecs.shape}\\nX.shape: {x.shape[1], x.shape[-1]}\")\n",
    "    assert unit_vecs.shape == (x.shape[1], x.shape[-1])\n",
    "    n_dim = x.ndim\n",
    "    shape = [d if i == 1 or i == n_dim-1 else 1 for i,d in enumerate(x.shape)]\n",
    "    return unit_vecs.view(*shape)\n",
    "\n",
    "def RoPE(W_Q, W_K, unit_vecs):\n",
    "    complex_W_Q = torch.view_as_complex(W_Q.float().reshape(*W_Q.shape[:-1], -1, 2))\n",
    "    complex_W_K = torch.view_as_complex(W_K.float().reshape(*W_K.shape[:-1], -1, 2))\n",
    "    # print(complex_W_Q.shape)\n",
    "    pos = brodcast(unit_vecs, complex_W_K)\n",
    "    embedded_W_Q = torch.view_as_real(complex_W_Q * pos).float().flatten(3)\n",
    "    embedded_W_K = torch.view_as_real(complex_W_K * pos).float().flatten(3)\n",
    "    return embedded_W_Q, embedded_W_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f6c1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIGURATIONS = {\n",
    "  \"DIM\": 3072,\n",
    "  \"FFN_DIM\": 8192,\n",
    "  \"N_LAYERS\": 28,\n",
    "  \"N_HEADS\": 24,\n",
    "  \"N_KV_HEADS\": 8,\n",
    "  \"VOCAB_SIZE\": 128256,\n",
    "  \"NORM_EPS\": 1e-5,\n",
    "  \"ROPE_THETA\": 500000,\n",
    "  \"MAX_BATCH_SIZE\": 4,\n",
    "  \"MAX_SEQ_LEN\": 6000,\n",
    "  \"N_KV_HEAD_REP\": 24 // 8,\n",
    "  \"HEAD_DIM\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d678e25e",
   "metadata": {},
   "source": [
    "# LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b01afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim, n_heads, head_dim, n_kv_heads, n_kv_heads_reps, max_batch_size, max_seq_len):\n",
    "        super().__init__()\n",
    "        self.W_Q = nn.Linear(dim, n_heads * head_dim, bias=False)\n",
    "        self.W_K = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n",
    "        self.W_V = nn.Linear(dim, n_kv_heads * head_dim, bias=False)\n",
    "\n",
    "        self.register_buffer('CACHE_K', torch.zeros(\n",
    "            (max_batch_size, max_seq_len, n_kv_heads, head_dim))\n",
    "        )\n",
    "        self.register_buffer('CACHE_V', torch.zeros(\n",
    "            (max_batch_size, max_seq_len, n_kv_heads, head_dim))\n",
    "        )\n",
    "\n",
    "        self.wo = nn.Linear(dim, dim)\n",
    "\n",
    "        self.n_heads = n_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.n_kv_heads = n_kv_heads\n",
    "        self.n_kv_heads_reps = n_kv_heads_reps\n",
    "\n",
    "\n",
    "    def forward(self,x, freq=None, start_pos=0, mask=None):\n",
    "        bhz, seq_len, _ = x.shape\n",
    "\n",
    "        query = self.W_Q(x).view(bhz, seq_len, self.n_heads, self.head_dim)\n",
    "        key = self.W_K(x).view(bhz, seq_len, self.n_kv_heads, self.head_dim)\n",
    "        value = self.W_V(x).view(bhz, seq_len, self.n_kv_heads, self.head_dim)\n",
    "\n",
    "        query, key = RoPE(query, key, freq)\n",
    "\n",
    "        self.CACHE_K[:bhz, start_pos:start_pos+seq_len] = key\n",
    "        self.CACHE_V[:bhz, start_pos:start_pos+seq_len] = value\n",
    "\n",
    "        keys = self.CACHE_K[:bhz, :start_pos+seq_len]\n",
    "        values = self.CACHE_V[:bhz, :start_pos+seq_len]\n",
    "\n",
    "        keys = torch.repeat_interleave(input=keys, repeats=self.n_kv_heads_reps, dim=-2)\n",
    "        values = torch.repeat_interleave(input=values, repeats=self.n_kv_heads_reps, dim=-2)\n",
    "\n",
    "        queries = query.transpose(1,2)\n",
    "        keys = keys.transpose(1,2)\n",
    "        values = values.transpose(1,2)\n",
    "        \n",
    "        out = F.scaled_dot_product_attention(queries, keys, values, attn_mask=mask)\n",
    "        out = out.transpose(1,2).contiguous().view(bhz, seq_len, -1)\n",
    "\n",
    "        return self.wo(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9fc577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim, norm_eps):\n",
    "        super().__init__()\n",
    "        self.norm_eps = norm_eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.norm_eps)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self._norm(x.float()).type_as(x)\n",
    "        return out * self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8997b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, ffn_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.w1 = nn.Linear(dim, ffn_dim, bias=False)\n",
    "        self.w3 = nn.Linear(dim, ffn_dim, bias=False)\n",
    "        self.w2 = nn.Linear(ffn_dim, dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd82bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer_Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.Attention_Norm = RMSNorm(dim=config[\"DIM\"], norm_eps=config[\"NORM_EPS\"])\n",
    "        self.FFN_Norm = RMSNorm(dim=config[\"DIM\"], norm_eps=config[\"NORM_EPS\"])\n",
    "        self.Attention = Attention(dim=config[\"DIM\"],\n",
    "                                   n_heads=config[\"N_HEADS\"],\n",
    "                                   head_dim=config[\"HEAD_DIM\"],\n",
    "                                   n_kv_heads=config[\"N_KV_HEADS\"],\n",
    "                                   n_kv_heads_reps=config[\"N_KV_HEAD_REP\"],\n",
    "                                   max_batch_size=config[\"MAX_BATCH_SIZE\"],\n",
    "                                   max_seq_len=config[\"MAX_SEQ_LEN\"])\n",
    "        self.FeedForward = FeedForward(dim=config[\"DIM\"],\n",
    "                                       ffn_dim=config[\"FFN_DIM\"])\n",
    "    def forward(self, x, freq, start_pos, mask):\n",
    "        shortcut = x\n",
    "        x = self.Attention_Norm(x)\n",
    "        x = self.Attention(x, freq, start_pos, mask)\n",
    "        x = x + shortcut\n",
    "\n",
    "        shortcut = x\n",
    "        x = self.FFN_Norm(x)\n",
    "        x = self.FeedForward(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af5202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLAMA_3(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tok_embedding = nn.Embedding(config[\"VOCAB_SIZE\"], config[\"DIM\"])\n",
    "        self.layers = nn.ModuleList()\n",
    "        for _ in range(config[\"N_LAYERS\"]):\n",
    "            self.layers.append(Transformer_Block(config))\n",
    "        self.norm = RMSNorm(config[\"DIM\"], config[\"NORM_EPS\"])\n",
    "        self.output = nn.Linear(config[\"DIM\"], config[\"VOCAB_SIZE\"], bias=False)\n",
    "\n",
    "        self.register_buffer(\n",
    "            'freq',\n",
    "            calculate_angles(\n",
    "                config[\"ROPE_THETA\"],\n",
    "                config[\"HEAD_DIM\"],\n",
    "                config[\"MAX_SEQ_LEN\"] * 2\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    def reset_cache(self):\n",
    "        for name, module in self.named_modules():\n",
    "            if hasattr(module, \"CACHE_K\"):\n",
    "                module.CACHE_K.zero_()\n",
    "            if hasattr(module, \"CACHE_V\"):\n",
    "                module.CACHE_V.zero_()\n",
    "\n",
    "    \n",
    "    def forward(self, tokens, start_pos):\n",
    "        bhz, seq_len = tokens.shape\n",
    "        x = self.tok_embedding(tokens)\n",
    "        freq = self.freq[start_pos : start_pos+seq_len]\n",
    "\n",
    "        mask = None\n",
    "        if seq_len > 1:\n",
    "            mask = torch.full((seq_len, seq_len), float(\"-inf\"), device=tokens.device) \n",
    "            mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, freq, start_pos, mask )\n",
    "        x = self.norm(x)\n",
    "        x = self.output(x).float()\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e91f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama = LLAMA_3(config=CONFIGURATIONS).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658010aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in llama.parameters())\n",
    "print(f\"Total Number of parameters: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bae4694",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size_bytes = total_params * 4\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"Total size of the model: {total_size_mb/1024} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc77b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Generate_Text(model, idx, max_tokens, context_size, start_pos):\n",
    "    for _ in range(max_tokens):\n",
    "        idx_cond = idx[:, -context_size:]\n",
    "        with torch.inference_mode():\n",
    "            logits = model(idx_cond, start_pos)\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_idx = torch.argmax(probs, dim=-1, keepdim=True)\n",
    "            idx = torch.cat((idx, next_idx), dim=-1)\n",
    "    return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c222ee",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47063c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    no_reserved_special_tokens = 256\n",
    "    pat_str = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: E501\n",
    "\n",
    "    def __init__(self, model_path):\n",
    "        mergeable_ranks = load_tiktoken_bpe(model_path)\n",
    "        num_base_tokens = len(mergeable_ranks)\n",
    "        special_tokens = [\n",
    "            \"<|begin_of_text|>\",\n",
    "            \"<|end_of_text|>\",\n",
    "            \"<|reserved_special_token_0|>\",\n",
    "            \"<|reserved_special_token_1|>\",\n",
    "            \"<|reserved_special_token_2|>\",\n",
    "            \"<|reserved_special_token_3|>\",\n",
    "            \"<|start_header_id|>\",\n",
    "            \"<|end_header_id|>\",\n",
    "            \"<|reserved_special_token_4|>\",\n",
    "            \"<|eot_id|>\",  # end of turn\n",
    "        ] + [f\"<|reserved_special_token_{i}|>\" for i in range(5, self.no_reserved_special_tokens - 5)]\n",
    "        \n",
    "        self.special_tokens = {\n",
    "            token: num_base_tokens + i for i, token in enumerate(special_tokens)\n",
    "        }\n",
    "\n",
    "        self.model = tiktoken.Encoding(\n",
    "            name=Path(model_path).name,\n",
    "            pat_str=self.pat_str,\n",
    "            mergeable_ranks=mergeable_ranks,\n",
    "            special_tokens=self.special_tokens\n",
    "        )\n",
    "        self.n_words = self.model.n_vocab\n",
    "        self.bos_id: int = self.special_tokens[\"<|begin_of_text|>\"]\n",
    "        self.eos_id: int = self.special_tokens[\"<|end_of_text|>\"]\n",
    "        self.pad_id: int = -1\n",
    "        self.stop_tokens = {\n",
    "            self.special_tokens[\"<|end_of_text|>\"],\n",
    "            self.special_tokens[\"<|eot_id|>\"],\n",
    "        }\n",
    "\n",
    "    def encode(\n",
    "        self,\n",
    "        s: str,\n",
    "        *,\n",
    "        bos: bool,\n",
    "        eos: bool,\n",
    "        allowed_special: Union[Literal[\"all\"], AbstractSet[str]] = set(),\n",
    "        disallowed_special: Union[Literal[\"all\"], Collection[str]] = (),\n",
    "        ):\n",
    "        TIKTOKEN_MAX_ENCODE_CHARS = 400_000\n",
    "        MAX_NO_WHITESPACES_CHARS = 25_000\n",
    "        substrs = (\n",
    "            substr\n",
    "            for i in range(0, len(s), TIKTOKEN_MAX_ENCODE_CHARS)\n",
    "            for substr in self._split_whitespaces_or_nonwhitespaces(\n",
    "                s[i : i + TIKTOKEN_MAX_ENCODE_CHARS], MAX_NO_WHITESPACES_CHARS\n",
    "            )\n",
    "        )\n",
    "        t: List[int] = []\n",
    "        for substr in substrs:\n",
    "            t.extend(\n",
    "                self.model.encode(\n",
    "                    substr,\n",
    "                    allowed_special=allowed_special,\n",
    "                    disallowed_special=disallowed_special,\n",
    "                )\n",
    "            )\n",
    "        if bos:\n",
    "            t.insert(0, self.bos_id)\n",
    "        if eos:\n",
    "            t.append(self.eos_id)\n",
    "        return t\n",
    "    def decode(self, t: Sequence[int]) -> str:\n",
    "        return self.model.decode(cast(List[int], t))\n",
    "\n",
    "    def _split_whitespaces_or_nonwhitespaces(self,\n",
    "        s: str, max_consecutive_slice_len: int\n",
    "    ):\n",
    "        current_slice_len = 0\n",
    "        current_slice_is_space = s[0].isspace() if len(s) > 0 else False\n",
    "        slice_start = 0\n",
    "\n",
    "        for i in range(len(s)):\n",
    "            is_now_space = s[i].isspace()\n",
    "\n",
    "            if current_slice_is_space ^ is_now_space:\n",
    "                current_slice_len = 1\n",
    "                current_slice_is_space = is_now_space\n",
    "            else:\n",
    "                current_slice_len += 1\n",
    "                if current_slice_len > max_consecutive_slice_len:\n",
    "                    yield s[slice_start:i]\n",
    "                    slice_start = i\n",
    "                    current_slice_len = 1\n",
    "        yield s[slice_start:]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ea995f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer(\"Weights/3B-instruct/original/tokenizer.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195f25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tok.encode(s=\"my name is sarthak\", bos=False, eos=False)\n",
    "z = torch.tensor([z], dtype=torch.long, device=device)\n",
    "d = Generate_Text(llama, z, 5, 10, 0)\n",
    "d = d.squeeze(dim=0)\n",
    "tok.decode(d.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12110fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import load_consolidated_pth_weights, fetch_context_from_web, clean_serp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388dd73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"./Weights/3B-instruct/original/consolidated.00.pth\"\n",
    "params = load_consolidated_pth_weights(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c12b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(params.keys())[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1750c39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def weight_injector(my_model, params):\n",
    "    device = next(my_model.parameters()).device\n",
    "    dtype = next(my_model.parameters()).dtype\n",
    "\n",
    "    my_model.tok_embedding.weight.data.copy_(\n",
    "        params[\"tok_embeddings.weight\"].to(device=device, dtype=dtype)\n",
    "    )\n",
    "\n",
    "    num_layers = len(my_model.layers)\n",
    "    for i in range(num_layers):\n",
    "        layer = my_model.layers[i]\n",
    "\n",
    "        layer.Attention.W_Q.weight.data.copy_(\n",
    "            params[f\"layers.{i}.attention.wq.weight\"].to(device=device, dtype=dtype)\n",
    "        )\n",
    "        layer.Attention.W_K.weight.data.copy_(\n",
    "            params[f\"layers.{i}.attention.wk.weight\"].to(device=device, dtype=dtype)\n",
    "        )\n",
    "        layer.Attention.W_V.weight.data.copy_(\n",
    "            params[f\"layers.{i}.attention.wv.weight\"].to(device=device, dtype=dtype)\n",
    "        )\n",
    "        layer.Attention.wo.weight.data.copy_(\n",
    "            params[f\"layers.{i}.attention.wo.weight\"].to(device=device, dtype=dtype)\n",
    "        )\n",
    "\n",
    "        if hasattr(layer.Attention.wo, \"bias\") and f\"layers.{i}.attention.wo.bias\" in params:\n",
    "            layer.Attention.wo.bias.data.copy_(\n",
    "                params[f\"layers.{i}.attention.wo.bias\"].to(device=device, dtype=dtype)\n",
    "            )\n",
    "\n",
    "        layer.FeedForward.w1.weight.data.copy_(\n",
    "            params[f\"layers.{i}.feed_forward.w1.weight\"].to(device=device, dtype=dtype)\n",
    "        )\n",
    "        layer.FeedForward.w3.weight.data.copy_(\n",
    "            params[f\"layers.{i}.feed_forward.w3.weight\"].to(device=device, dtype=dtype)\n",
    "        )\n",
    "        layer.FeedForward.w2.weight.data.copy_(\n",
    "            params[f\"layers.{i}.feed_forward.w2.weight\"].to(device=device, dtype=dtype)\n",
    "        )\n",
    "\n",
    "        layer.Attention_Norm.weight.data.copy_(\n",
    "            params[f\"layers.{i}.attention_norm.weight\"].to(device=device, dtype=dtype)\n",
    "        )\n",
    "        layer.FFN_Norm.weight.data.copy_(\n",
    "            params[f\"layers.{i}.ffn_norm.weight\"].to(device=device, dtype=dtype)\n",
    "        )\n",
    "\n",
    "    my_model.norm.weight.data.copy_(\n",
    "        params[\"norm.weight\"].to(device=device, dtype=dtype)\n",
    "    )\n",
    "\n",
    "    out_w = params[\"output.weight\"]\n",
    "    if my_model.output.weight.shape == out_w.shape:\n",
    "        my_model.output.weight.data.copy_(out_w.to(device=device, dtype=dtype))\n",
    "    elif my_model.output.weight.shape[::-1] == out_w.shape:\n",
    "        my_model.output.weight.data.copy_(out_w.T.to(device=device, dtype=dtype))\n",
    "    else:\n",
    "        raise ValueError(f\"Output weight shape mismatch: model={my_model.output.weight.shape}, ckpt={out_w.shape}\")\n",
    "\n",
    "    print(\"All weights successfully injected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f327b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_injector(llama, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e5790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model, idx, context_len, max_new_tok, top_k,\n",
    "    temp=0.1,\n",
    "    eos_id=128001,\n",
    "    eot_id=128009,\n",
    "    eom_id=128008,\n",
    "):\n",
    "    model.eval()\n",
    "    model.reset_cache()\n",
    "    for tok_no in range(max_new_tok):\n",
    "        if tok_no == 0:\n",
    "            idx_cond = idx[:, -context_len:] if idx.shape[1] > context_len else idx\n",
    "            start_pos = 0\n",
    "        else:\n",
    "            idx_cond = idx[:, -1:]\n",
    "            start_pos = idx.shape[1] - 1\n",
    "        print(f\"\\rToken {tok_no}: seq_len={idx_cond.shape[1]}, start_pos={start_pos}\", end=\"\")\n",
    "        with torch.inference_mode():\n",
    "            logits = model(idx_cond, start_pos)\n",
    "        logits = logits[:, -1, :]\n",
    "\n",
    "        if top_k > 0:\n",
    "            top_k_logits, _ = torch.topk(logits, top_k)\n",
    "            min_val = top_k_logits[:, -1:]\n",
    "            logits = torch.where(logits < min_val, float(\"-inf\"), logits)\n",
    "\n",
    "        logits /= max(temp, 1e-8)\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        preds = torch.multinomial(probs, 1)\n",
    "\n",
    "        next_tok = preds.item()\n",
    "\n",
    "        if next_tok in {eos_id, eot_id, eom_id}:\n",
    "            print(f\"\\n[Stop token {next_tok} reached at step {tok_no}]\")\n",
    "            break\n",
    "\n",
    "        idx = torch.cat([idx, preds], dim=1)\n",
    "    return idx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7910ccd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_and_tokenize(system_prompt, user_prompt, context=False, device=\"cuda\"):\n",
    "    if context == True:\n",
    "        cntx = clean_serp_data(fetch_context_from_web(user_prompt, n_results=1))\n",
    "        print(cntx)\n",
    "        user_prompt = f\"Use the following context to answer:\\n{cntx.strip()}\\n\\nQuestion: {user_prompt.strip()}\"\n",
    "\n",
    "    prompt = (\n",
    "        \"<|start_header_id|>system<|end_header_id|>\\n\"\n",
    "        f\"{system_prompt.strip()}\\n\"\n",
    "        \"<|eot_id|>\\n\"\n",
    "        \"<|start_header_id|>user<|end_header_id|>\\n\"\n",
    "        f\"{user_prompt.strip()}\\n\"\n",
    "        \"<|eot_id|>\\n\"\n",
    "        \"<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "    )\n",
    "\n",
    "    input_ids = tok.encode(prompt, bos=True, eos=False, allowed_special=\"all\")\n",
    "    return torch.tensor([input_ids], device=device)\n",
    "    # return tok.decode(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e684f2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When context is provided, summarize it in your own words rather than copying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929da2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"You are a concise and factual AI assistant, and your name is Blue. You have to answer the Question asked by user.\"\n",
    "user_prompt = \"hi how are you\"\n",
    "idx = format_and_tokenize(system_prompt, user_prompt, context=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8876cfef",
   "metadata": {},
   "source": [
    "# Decoding Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dda9616",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = generate(llama, idx, context_len=2048, max_new_tok=50, top_k=50, temp=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0641dfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_assistant_output(out, tokenizer):\n",
    "    text = tokenizer.decode(out.squeeze().tolist())\n",
    "    segments = text.split(\"<|start_header_id|>assistant<|end_header_id|>\")\n",
    "\n",
    "    if len(segments) <= 1:\n",
    "        return text.strip()\n",
    "\n",
    "    assistants = []\n",
    "    for seg in segments[1:]:\n",
    "        assistants.append(seg.split(\"<|eot_id|>\")[0].strip())\n",
    "\n",
    "    return assistants[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57067520",
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_response = extract_assistant_output(out, tok)\n",
    "print(assistant_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3818e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = out.squeeze()\n",
    "x = tok.decode(out.tolist())\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4dd1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326a4288",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [78191]\n",
    "print(tok.decode(r))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fda95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \"<|start_header_id|>\"\n",
    "print(tok.encode(y, bos=False, eos=False, allowed_special=\"all\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
